{"pages":[{"title":"","text":"个人简介 骚气自我介绍： “本人学识渊博、经验丰富，代码风骚、效率恐怖，HTML5/CSS3、nextJS、vueJS、java、SpringBoot、SpringCloud、K8S、Docker无不精通，熟练掌握各种框架，深山苦练20余年，一天只睡4个小时，千里之外定位问题，瞬息之间修复上线。身体强壮、健步如飞，可连续编程100小时不休息，讨论技术方案5小时不喝水，上至带项目、出方案，下至盗账号、威胁PM，啥都能干。泡面矿泉水已准备好，学校不支持编程已辍学，家人不支持编程已断绝关系，老婆不支持编程已离婚，小孩不支持编程已送孤儿院，备用电源万兆光纤永不断电断网，门口已埋雷无人打扰。“ 善恶终有报,天道好轮回。不信抬头看,苍天饶过谁。无论何时何地，我们都要保持一颗积极乐观、善良感恩的心。但行好事莫问前程，永远年轻，永远热内盈眶，永远保持正能量。💪💪💪💪💪💪冲鸭！！！！ -&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;个人信息：计算机科学与技术专业从事JAVA后端开发、前端也行、带队、喜欢搞事情码畜一枚坚信代码改变世界 计划2020计划 时间轴记录","link":"/about/index.html"},{"title":"","text":"唐艺昕 李沁 李一桐 gakki 图片搜集于互联网，侵权请留言，马上处理😊。","link":"/album/index.html"},{"title":"","text":"来而不往非礼也畅所欲言，有留必应","link":"/message/index.html"},{"title":"","text":"&nbsp;&nbsp;听听音乐 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出。更多音乐分享请查看歌单。 &nbsp;&nbsp;看看视频 ->点击以下条目开始播放视频,向下滑动查看更多","link":"/media/index.html"},{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 本站友链信息如下，申请友链前请先添加本站信息： 网站图标：https://removeif.github.io/images/avatar.jpg 网站名称：辣椒の酱 网站地址：https://removeif.github.io 网站简介：后端开发，技术分享 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"音乐歌单收藏","text":"--- 温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"清茶时光 tips：github登录后按时间正序查看、可点赞加❤️、本插件地址..「+99次查看」 var gitalk = new Gitalk({ clientID: '53ef521372c3dc152526', clientSecret: 'ac464204f70ae7097de2ea7e7721ac5c2c9fca3f', id: '666666', repo: 'blog-database', owner: 'xincan', admin: \"xincan\", createIssueManually: true, distractionFreeMode: false }) gitalk.render('comment-container1')","link":"/self-talking/index.html"}],"posts":[{"title":"加密文章测试","text":"嗨，请准确无误地输入密码查看哟（密码：123456）！ 42401e656a30ed09e8e3b0fbe2fbd18ae1b8c2138de0b629bd1233cbc0cc80918b020088b54c79621b8ef9a9af7b0f06b4f358e8c86ef99d097fb161589581b453752da7da2e21aa4d35b94c835c62d1204ab327d5d9241d6be3c6b4b1e68d3697229826ffa52b53920e92cbe496af209269f803aa7450ba136338b0e891ba54f6fe48d0d23c1ba5b981305fdee3badd054b7ad20712af650adf010b25c3fa117b769979c2ef7a2ccccee14b24ed84530b911e7b68c43d9f4592ba8687b2c7fb","link":"/posts/7e709353/"},{"title":"centos+K8S群集监控环境构建","text":"虚拟机配置准备三台虚拟机，每台虚机请参照centos基础环境构建、centos构建docker基础环境进行安装构建 一、 三台虚拟机配置表 服务器配置 服务器IP 域名 别名 服务器类别 登录用户 登录密码 CPU 内存 192.168.1.55 master55.xincan.cn master55 master root v*****t 4核 4G 192.168.1.56 slave56.xincan.cn slave56 slave root v*****t 2核 2G 192.168.1.57 slave57.xincan.cn slave57 slave root v*****t 2核 2G 工具版本 docker version: v19.03.11Kubernetes version: v1.18.2 参考地址kubernetes监控中级方案 二、 修改虚机域名依次修改各个虚拟机域名为master55.xincan.cn、 slave56.xincan.cn、 slave57.xincan.cn script1234567891011121314// master55服务器[root@localhost ~]# vi /etc/hostnamemaster55.xincan.cn[root@localhost ~]#// slave56服务器[root@localhost ~]# vi /etc/hostnameslave56.xincan.cn[root@localhost ~]#// slave57服务器[root@localhost ~]# vi /etc/hostnameslave57.xincan.cn[root@localhost ~]# 三、 配置三台机器互相用域名、别名访问修改/etc/hosts文件设置，3台服务器同时增加如下代码192.168.1.55 master55.xincan.cn master55192.168.1.56 slave56.xincan.cn slave56192.168.1.57 slave57.xincan.cn slave57 script12345678[root@localhost /]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.55 master55.xincan.cn master55192.168.1.56 slave56.xincan.cn slave56192.168.1.57 slave57.xincan.cn slave57[root@localhost /]# 四、 重启三台虚拟机，执行reboot分别链接3台服务器，由之前的localhost已经改成服务器别名 script12345678 // master55[root@master55 ~]# // master55[root@slave56 ~]# // master55[root@slave57 ~]# 五、 三台服务器时间同步 3台服务器都安装ntp，提示Complete!则安装成功 script12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@localhost ~]# sudo yum install -y ntpLoaded plugins: fastestmirrorDetermining fastest mirrors * base: mirrors.huaweicloud.com * extras: mirrors.huaweicloud.com * updates: mirrors.ustc.edu.cnbase | 3.6 kB 00:00:00extras | 2.9 kB 00:00:00updates | 2.9 kB 00:00:00(1/4): base/7/x86_64/group_gz | 153 kB 00:00:00(2/4): extras/7/x86_64/primary_db | 194 kB 00:00:00(3/4): updates/7/x86_64/primary_db | 2.1 MB 00:00:01(4/4): base/7/x86_64/primary_db | 6.1 MB 00:00:05Resolving Dependencies--&gt; Running transaction check---&gt; Package ntp.x86_64 0:4.2.6p5-29.el7.centos will be installed--&gt; Processing Dependency: ntpdate = 4.2.6p5-29.el7.centos for package: ntp-4.2.6p5-29.el7.centos.x86_64--&gt; Processing Dependency: libopts.so.25()(64bit) for package: ntp-4.2.6p5-29.el7.centos.x86_64--&gt; Running transaction check---&gt; Package autogen-libopts.x86_64 0:5.18-5.el7 will be installed---&gt; Package ntpdate.x86_64 0:4.2.6p5-29.el7.centos will be installed--&gt; Finished Dependency ResolutionDependencies Resolved===================================================================================================================================================== Package Arch Version Repository Size=====================================================================================================================================================Installing: ntp x86_64 4.2.6p5-29.el7.centos base 548 kInstalling for dependencies: autogen-libopts x86_64 5.18-5.el7 base 66 k ntpdate x86_64 4.2.6p5-29.el7.centos base 86 kTransaction Summary=====================================================================================================================================================Install 1 Package (+2 Dependent packages)Total download size: 701 kInstalled size: 1.6 MDownloading packages:warning: /var/cache/yum/x86_64/7/base/packages/ntpdate-4.2.6p5-29.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY- ETAPublic key for ntpdate-4.2.6p5-29.el7.centos.x86_64.rpm is not installed(1/3): ntpdate-4.2.6p5-29.el7.centos.x86_64.rpm | 86 kB 00:00:00(2/3): autogen-libopts-5.18-5.el7.x86_64.rpm | 66 kB 00:00:00(3/3): ntp-4.2.6p5-29.el7.centos.x86_64.rpm | 548 kB 00:00:00-----------------------------------------------------------------------------------------------------------------------------------------------------Total 956 kB/s | 701 kB 00:00:00Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7Importing GPG key 0xF4A80EB5: Userid : \"CentOS-7 Key (CentOS 7 Official Signing Key) &lt;security@centos.org&gt;\" Fingerprint: 6341 ab27 53d7 8a78 a7c2 7bb1 24c6 a8a7 f4a8 0eb5 Package : centos-release-7-6.1810.2.el7.centos.x86_64 (@anaconda) From : /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : autogen-libopts-5.18-5.el7.x86_64 1/3 Installing : ntpdate-4.2.6p5-29.el7.centos.x86_64 2/3 Installing : ntp-4.2.6p5-29.el7.centos.x86_64 3/3 Verifying : ntp-4.2.6p5-29.el7.centos.x86_64 1/3 Verifying : ntpdate-4.2.6p5-29.el7.centos.x86_64 2/3 Verifying : autogen-libopts-5.18-5.el7.x86_64 3/3Installed: ntp.x86_64 0:4.2.6p5-29.el7.centosDependency Installed: autogen-libopts.x86_64 0:5.18-5.el7 ntpdate.x86_64 0:4.2.6p5-29.el7.centosComplete![root@localhost ~]# 3台服务器同时设置，查看当前系统时间、并设置当前时间为上海 script123456[root@localhost /]# dateThu Jun 4 05:28:48 UTC 2020[root@localhost /]# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime[root@localhost /]# dateThu Jun 4 13:29:01 CST 2020[root@localhost /]# master55配置server ntp.aliyun.com iburst，然后通过sudo systemctl start ntpd启动服务，稍等一会执行ntpq -p查看是否同步，如果出现前面的*则，同步成功 systemctl start ntpd &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;启动ntpsystemctl restart ntpd &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;重启ntpsystemctl enable ntpd.service &nbsp;&nbsp;开机启动ntpdc -c loopinfo &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;查看与时间同步服务器的时间差 script123456789101112131415161718[root@master55 /]# vi /etc/ntp.conf# server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburstserver ntp.aliyun.com iburst[root@master55 /]#[root@master55 /]# sudo systemctl start ntpd[root@master55 /]# systemctl enable ntpd.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.[root@localhost /]# ntpq -p remote refid st t when poll reach delay offset jitter==============================================================================*203.107.6.88 10.137.38.86 2 u 37 64 1 53.476 -5.668 2.224[root@master55 /]# slave56、slave57配置master55域名地址server master55.xincan.cn iburst，然后通过sudo systemctl start ntpd启动服务，稍等一会执行ntpq -p查看是否同步，如果出现前面的*则，同步成功 slave56设置 script123456789101112131415161718[root@slave56 /]# vi /etc/ntp.conf# server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburstserver master55.xincan.cn iburst[root@slave56 /]#[root@slave56 /]# sudo systemctl start ntpd[root@slave56 /]# systemctl enable ntpd.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.[root@slave56 /]# ntpq -p remote refid st t when poll reach delay offset jitter==============================================================================*master55.xincan 203.107.6.88 3 u 12 64 1 0.367 10.659 0.054[root@slave56 /]# slave57设置 script123456789101112131415161718[root@slave57 /]# vi /etc/ntp.conf# server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburstserver master55.xincan.cn iburst[root@slave57 /]#[root@slave57 /]# sudo systemctl start ntpd[root@slave57 /]# systemctl enable ntpd.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.[root@slave57 /]# ntpq -p remote refid st t when poll reach delay offset jitter==============================================================================*master55.xincan 203.107.6.88 3 u 12 64 1 0.367 10.659 0.054[root@slave57 /]# 六、 三台服务器同时配置kubernetes网桥设置网桥为值为1 master55设置 script1234567891011121314151617181920212223242526272829303132[root@master55 /]# sudo bash -c 'cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward=1EOF'[root@master55 /]#[root@master55 /]# sysctl --system* Applying /usr/lib/sysctl.d/00-system.conf ...net.bridge.bridge-nf-call-ip6tables = 0net.bridge.bridge-nf-call-iptables = 0net.bridge.bridge-nf-call-arptables = 0* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...kernel.yama.ptrace_scope = 0* Applying /usr/lib/sysctl.d/50-default.conf ...kernel.sysrq = 16kernel.core_uses_pid = 1net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.all.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0net.ipv4.conf.all.accept_source_route = 0net.ipv4.conf.default.promote_secondaries = 1net.ipv4.conf.all.promote_secondaries = 1fs.protected_hardlinks = 1fs.protected_symlinks = 1* Applying /etc/sysctl.d/99-sysctl.conf ...* Applying /etc/sysctl.d/k8s.conf ...net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1* Applying /etc/sysctl.conf ...[root@master55 /]# slave56设置 script1234567891011121314151617181920212223242526272829303132[root@slave56 /]# sudo bash -c 'cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward=1EOF'[root@slave56 /]#[root@slave56 /]# sysctl --system* Applying /usr/lib/sysctl.d/00-system.conf ...net.bridge.bridge-nf-call-ip6tables = 0net.bridge.bridge-nf-call-iptables = 0net.bridge.bridge-nf-call-arptables = 0* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...kernel.yama.ptrace_scope = 0* Applying /usr/lib/sysctl.d/50-default.conf ...kernel.sysrq = 16kernel.core_uses_pid = 1net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.all.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0net.ipv4.conf.all.accept_source_route = 0net.ipv4.conf.default.promote_secondaries = 1net.ipv4.conf.all.promote_secondaries = 1fs.protected_hardlinks = 1fs.protected_symlinks = 1* Applying /etc/sysctl.d/99-sysctl.conf ...* Applying /etc/sysctl.d/k8s.conf ...net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1* Applying /etc/sysctl.conf ...[root@slave56 /]# slave57设置 script1234567891011121314151617181920212223242526272829303132[root@slave57 /]# sudo bash -c 'cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward=1EOF'[root@slave57 /]#[root@slave57 /]# sysctl --system* Applying /usr/lib/sysctl.d/00-system.conf ...net.bridge.bridge-nf-call-ip6tables = 0net.bridge.bridge-nf-call-iptables = 0net.bridge.bridge-nf-call-arptables = 0* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...kernel.yama.ptrace_scope = 0* Applying /usr/lib/sysctl.d/50-default.conf ...kernel.sysrq = 16kernel.core_uses_pid = 1net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.all.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0net.ipv4.conf.all.accept_source_route = 0net.ipv4.conf.default.promote_secondaries = 1net.ipv4.conf.all.promote_secondaries = 1fs.protected_hardlinks = 1fs.protected_symlinks = 1* Applying /etc/sysctl.d/99-sysctl.conf ...* Applying /etc/sysctl.d/k8s.conf ...net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1* Applying /etc/sysctl.conf ...[root@slave57 /]# 七、 三台服务器同时配置kubernetes下载源，关闭SELinux master55设置 script1234567891011121314[root@master55 /]# sudo bash -c 'cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF'[root@master55 /]#[root@master55 /]# sudo setenforce 0setenforce: SELinux is disabled[root@master55 /]# slave56设置 script1234567891011121314[root@slave56 /]# sudo bash -c 'cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF'[root@slave56 /]#setenforce: SELinux is disabled[root@slave56 /]# sudo setenforce 0[root@slave56 /]# slave57设置 script1234567891011121314[root@slave57 /]# sudo bash -c 'cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF'[root@slave57 /]#[root@slave57 /]# sudo setenforce 0setenforce: SELinux is disabled[root@slave57 /]# 八、 查看三台服务器下载源列表，如下成功script1234drwxr-xr-x. 2 root root 187 Jun 16 13:05 backup-rw-r--r--. 1 root root 2523 Jun 16 2018 CentOS-Base.repo-rw-r--r--. 1 root root 2424 Oct 19 2019 docker-ce.repo-rw-r--r--. 1 root root 272 Jun 16 16:34 kubernetes.repo 九、 三台服务器设置免密登录1：manager节点执行：ssh-keygen -t rsa 一路回车到结束，在/root/.ssh/下面会生成一个公钥文件id_rsa.pub script1234567891011121314151617181920212223[root@master55 /]# ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Created directory '/root/.ssh'.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:I0PR5fMj01uGGb1Z3pbRFjzwTIFb2ONyZ1M2I9OVTNY root@master55.xincan.cnThe key's randomart image is:+---[RSA 2048]----+| .. .. .X**|| ... *=%E|| . o . B=X|| . + * X*|| o S o * Bo*|| o . o = . || . || || |+----[SHA256]-----+[root@master55 /]# 将公钥追加到authorized_keys script12345678[root@master55 /]# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[root@master55 /]# cd ~/.ssh/[root@master55 .ssh]# lltotal 12-rw-r--r-- 1 root root 405 Jun 16 17:03 authorized_keys-rw------- 1 root root 1675 Jun 16 17:01 id_rsa-rw-r--r-- 1 root root 405 Jun 16 17:01 id_rsa.pub[root@master55 .ssh]# 修改authorized_keys权限： script12[root@master55 /]# chmod 600 ~/.ssh/authorized_keys[root@master55 /]# 将~/.ssh从master55节点分发到slave56、slave57节点，执行：scp -r ~/.ssh/ root@slave56:~/.ssh/scp -r ~/.ssh/ root@slave57:~/.ssh/过程中需要填写yes，然后提示输入slave56、slave57两个节点的登录密码 script1234567891011121314151617181920212223[root@master55 .ssh]# scp -r ~/.ssh/ root@slave56:~/.ssh/The authenticity of host 'slave56 (192.168.1.56)' can't be established.ECDSA key fingerprint is SHA256:KhL6Vyv6q5fHHcZ3+xoLn6W/mZ7SBAFD+n/TCXEHtSM.ECDSA key fingerprint is MD5:71:35:87:3d:ff:73:04:fc:d7:a2:07:30:68:b8:62:5b.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added 'slave56,192.168.1.56' (ECDSA) to the list of known hosts.root@slave56's password:id_rsa 100% 1675 1.1MB/s 00:00id_rsa.pub 100% 405 282.4KB/s 00:00authorized_keys 100% 405 277.0KB/s 00:00known_hosts 100% 182 104.6KB/s 00:00[root@master55 .ssh]# scp -r ~/.ssh/ root@slave57:~/.ssh/The authenticity of host 'slave57 (192.168.1.57)' can't be established.ECDSA key fingerprint is SHA256:Gfz+xXR217Yb2ZWOIMsRzSe+iynRvpxLnt98cI4kBRA.ECDSA key fingerprint is MD5:8b:1d:cd:1d:24:79:de:80:c3:53:7c:d3:87:e0:d4:96.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added 'slave57,192.168.1.57' (ECDSA) to the list of known hosts.root@slave57's password:id_rsa 100% 1675 1.0MB/s 00:00id_rsa.pub 100% 405 304.6KB/s 00:00authorized_keys 100% 405 352.7KB/s 00:00known_hosts 100% 364 271.2KB/s 00:00[root@master55 .ssh]# 验证master55、slave56、slave57三个节点免密登录master55节点验证 script1234567891011[root@master55 /]# ssh root@slave56Last login: Tue Jun 16 15:11:10 2020 from 192.168.1.182[root@slave56 ~]# exitlogoutConnection to slave56 closed.[root@master55 /]# ssh root@slave57Last login: Tue Jun 16 15:11:23 2020 from 192.168.1.182[root@slave57 ~]# exitlogoutConnection to slave57 closed.[root@master55 /]# slave56节点验证,第一次链接需要输入目标服务密码，后续则不用 script12345678910111213141516171819202122232425262728293031[root@slave56 ~]# ssh root@master55The authenticity of host 'master55 (192.168.1.55)' can't be established.ECDSA key fingerprint is SHA256:Dv4+42UAUC3FCEqZjwxJECtUHMgAYUtD2UsRASyffFw.ECDSA key fingerprint is MD5:fe:0b:32:39:20:9c:e1:3e:67:b7:3d:42:a1:22:df:2a.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added 'master55,192.168.1.55' (ECDSA) to the list of known hosts.Last login: Tue Jun 16 15:58:21 2020 from 192.168.1.182[root@master55 ~]# exitlogoutConnection to master55 closed.[root@slave56 ~]# ssh root@master55Last login: Tue Jun 16 17:17:38 2020 from 192.168.1.56[root@master55 ~]# exitlogoutConnection to master55 closed.[root@slave56 ~]# ssh root@slave57The authenticity of host 'slave57 (192.168.1.57)' can't be established.ECDSA key fingerprint is SHA256:Gfz+xXR217Yb2ZWOIMsRzSe+iynRvpxLnt98cI4kBRA.ECDSA key fingerprint is MD5:8b:1d:cd:1d:24:79:de:80:c3:53:7c:d3:87:e0:d4:96.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added 'slave57,192.168.1.57' (ECDSA) to the list of known hosts.Last login: Tue Jun 16 17:15:27 2020 from 192.168.1.55[root@slave57 ~]# exitlogoutConnection to slave57 closed.[root@slave56 ~]# ssh root@slave57Last login: Tue Jun 16 17:17:59 2020 from 192.168.1.56[root@slave57 ~]# exitlogoutConnection to slave57 closed.[root@slave56 ~]# slave57节点验证,第一次链接需要输入目标服务密码，后续则不用 script123456789101112131415161718192021[root@slave57 /]# ssh root@master55The authenticity of host 'master55 (192.168.1.55)' can't be established.ECDSA key fingerprint is SHA256:Dv4+42UAUC3FCEqZjwxJECtUHMgAYUtD2UsRASyffFw.ECDSA key fingerprint is MD5:fe:0b:32:39:20:9c:e1:3e:67:b7:3d:42:a1:22:df:2a.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added 'master55,192.168.1.55' (ECDSA) to the list of known hosts.Last login: Tue Jun 16 17:17:42 2020 from 192.168.1.56[root@master55 ~]# exitlogoutConnection to master55 closed.[root@slave57 yum.repos.d]# ssh root@master55Last login: Tue Jun 16 17:19:31 2020 from 192.168.1.57[root@master55 ~]# exitlogoutConnection to master55 closed.[root@slave57 yum.repos.d]# ssh root@slave56Last login: Tue Jun 16 17:15:11 2020 from 192.168.1.55[root@slave56 ~]# exitlogoutConnection to slave56 closed.[root@slave57 /]# 十、 三台服务器同时安装kubelet kubeadm kubectl安装完成之后，启动，并设置开机启动 script12sudo yum install -y kubelet-1.18.2 kubeadm-1.18.2 kubectl-1.18.2systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet 十一、以master55为Kubernetes主节点进行Kubernetes初始化 执行命令进行初始化sudo kubeadm init &minus;&minus;image-repository registry.aliyuncs.com/google_containers &minus;&minus;kubernetes-version v1.18.2 &minus;&minus;apiserver-advertise-address 192.168.1.55 &minus;&minus;pod-network-cidr=10.244.0.0/16 &minus;&minus;token-ttl 0 script123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[root@master55 /]# sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.2 --apiserver-advertise-address 192.168.1.55 --pod-network-cidr=10.244.0.0/16 --token-ttl 0W0616 17:24:47.742105 8831 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io][init] Using Kubernetes version: v1.18.2[preflight] Running pre-flight checks[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"[kubelet-start] Starting the kubelet[certs] Using certificateDir folder \"/etc/kubernetes/pki\"[certs] Generating \"ca\" certificate and key[certs] Generating \"apiserver\" certificate and key[certs] apiserver serving cert is signed for DNS names [master55.xincan.cn kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.55][certs] Generating \"apiserver-kubelet-client\" certificate and key[certs] Generating \"front-proxy-ca\" certificate and key[certs] Generating \"front-proxy-client\" certificate and key[certs] Generating \"etcd/ca\" certificate and key[certs] Generating \"etcd/server\" certificate and key[certs] etcd/server serving cert is signed for DNS names [master55.xincan.cn localhost] and IPs [192.168.1.55 127.0.0.1 ::1][certs] Generating \"etcd/peer\" certificate and key[certs] etcd/peer serving cert is signed for DNS names [master55.xincan.cn localhost] and IPs [192.168.1.55 127.0.0.1 ::1][certs] Generating \"etcd/healthcheck-client\" certificate and key[certs] Generating \"apiserver-etcd-client\" certificate and key[certs] Generating \"sa\" key and public key[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"[kubeconfig] Writing \"admin.conf\" kubeconfig file[kubeconfig] Writing \"kubelet.conf\" kubeconfig file[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file[kubeconfig] Writing \"scheduler.conf\" kubeconfig file[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"[control-plane] Creating static Pod manifest for \"kube-apiserver\"[control-plane] Creating static Pod manifest for \"kube-controller-manager\"W0616 17:29:47.640484 8831 manifests.go:225] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\"[control-plane] Creating static Pod manifest for \"kube-scheduler\"W0616 17:29:47.646613 8831 manifests.go:225] the default kube-apiserver authorization-mode is \"Node,RBAC\"; using \"Node,RBAC\"[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s[apiclient] All control plane components are healthy after 31.505848 seconds[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace[kubelet] Creating a ConfigMap \"kubelet-config-1.18\" in namespace kube-system with the configuration for the kubelets in the cluster[upload-certs] Skipping phase. Please see --upload-certs[mark-control-plane] Marking the node master55.xincan.cn as control-plane by adding the label \"node-role.kubernetes.io/master=''\"[mark-control-plane] Marking the node master55.xincan.cn as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: 991hr9.scqkkyphn1cjjcl7[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.1.55:6443 --token 991hr9.scqkkyphn1cjjcl7 \\ --discovery-token-ca-cert-hash sha256:1dcf2607e09f83160ce9bc99a941d9a6bd74e99b6b8d3adb63af800ffee19baf[root@master55 /]# 根据初始化提示，在master55节点上执行如下命令mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config script1234[root@master55 /]# mkdir -p $HOME/.kube[root@master55 /]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config[root@master55 /]# sudo chown $(id -u):$(id -g) $HOME/.kube/config[root@master55 /]# 根据初始化提示，在slave56、slave57节点上分别执行如下命令kubeadm join 192.168.1.55:6443 &minus;&minus;token 991hr9.scqkkyphn1cjjcl7 &minus;&minus;discovery-token-ca-cert-hash sha256:1dcf2607e09f83160ce9bc99a941d9a6bd74e99b6b8d3adb63af800ffee19baf script123456789101112131415W0616 17:50:09.914108 4585 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.[preflight] Running pre-flight checks[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.18\" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"[kubelet-start] Starting the kubelet[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 十二、Kubernetes命令自动补全（墙裂建议）SCRIPT12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667[root@master55 ~]# yum install -y epel-release bash-completionLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.comkubernetes/signature | 454 B 00:00:00kubernetes/signature | 1.4 kB 00:00:00 !!!Resolving Dependencies--&gt; Running transaction check---&gt; Package bash-completion.noarch 1:2.1-6.el7 will be updated---&gt; Package bash-completion.noarch 1:2.1-8.el7 will be an update---&gt; Package epel-release.noarch 0:7-11 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved============================================================================================================================================================================================== Package Arch Version Repository Size==============================================================================================================================================================================================Installing: epel-release noarch 7-11 extras 15 kUpdating: bash-completion noarch 1:2.1-8.el7 base 87 kTransaction Summary==============================================================================================================================================================================================Install 1 PackageUpgrade 1 PackageTotal download size: 101 kDownloading packages:No Presto metadata available for base(1/2): bash-completion-2.1-8.el7.noarch.rpm | 87 kB 00:00:00(2/2): epel-release-7-11.noarch.rpm | 15 kB 00:00:00----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Total 225 kB/s | 101 kB 00:00:00Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : epel-release-7-11.noarch 1/3 Updating : 1:bash-completion-2.1-8.el7.noarch 2/3 Cleanup : 1:bash-completion-2.1-6.el7.noarch 3/3 Verifying : 1:bash-completion-2.1-8.el7.noarch 1/3 Verifying : epel-release-7-11.noarch 2/3 Verifying : 1:bash-completion-2.1-6.el7.noarch 3/3Installed: epel-release.noarch 0:7-11Updated: bash-completion.noarch 1:2.1-8.el7Complete![root@master55 ~]# source /usr/share/bash-completion/bash_completion[root@master55 ~]#[root@master55 ~]# source &lt;(kubectl completion bash)[root@master55 ~]#[root@master55 ~]# echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrc// 输入kubectl, 双击Tab[root@master55 ~]# kubectlalpha apply certificate convert delete edit get options proxy scale uncordonannotate attach cluster-info cordon describe exec kustomize patch replace set versionapi-resources auth completion cp diff explain label plugin rollout taint waitapi-versions autoscale config create drain expose logs port-forward run top[root@master55 ~]# kubectl 十三、k8s查看所有节点master55节点查看所有节点，当前三台服务器的状态都是NotReady script123456[root@master55 /]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster55.xincan.cn NotReady master 16h v1.18.3slave56.xincan.cn NotReady &lt;none&gt; 15h v1.18.3slave57.xincan.cn NotReady &lt;none&gt; 16h v1.18.3[root@master55 /]# 十四、k8s查看所有命名空间下所有pod发现coredns一直处在pending状态，需要安装k8s网络插件 script123456789101112[root@master55 /]# kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-7ff77c879f-hz59h 0/1 Pending 0 16hkube-system coredns-7ff77c879f-kkdpn 0/1 Pending 0 16hkube-system etcd-master55.xincan.cn 1/1 Running 2 16hkube-system kube-apiserver-master55.xincan.cn 1/1 Running 2 16hkube-system kube-controller-manager-master55.xincan.cn 1/1 Running 2 16hkube-system kube-proxy-kdxlv 1/1 Running 2 16hkube-system kube-proxy-mxm5n 1/1 Running 2 16hkube-system kube-proxy-sdnxb 1/1 Running 2 15hkube-system kube-scheduler-master55.xincan.cn 1/1 Running 2 16h[root@master55 /]# 十五、k8s安装网络插件我们这里选取calico网络插件（提供企业级支持） 在master55节点上创建文件夹，用于存放下载的网络插件，我这里下载的是calico-3.13.1.yaml script123456789101112131415161718[root@master55 /]# mkdir k8s[root@master55 /]# cd k8s/[root@master55 k8s]# mkdir calico &amp;&amp; cd calico[root@master55 calico]# wget https://kuboard.cn/install-script/calico/calico-3.13.1.yaml--2020-06-17 17:42:44-- https://kuboard.cn/install-script/calico/calico-3.13.1.yamlResolving kuboard.cn (kuboard.cn)... 119.3.92.138, 122.112.240.69Connecting to kuboard.cn (kuboard.cn)|119.3.92.138|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 21079 (21K) [application/octet-stream]Saving to: ‘calico-3.13.1.yaml’100%[====================================================================================================================================================&gt;] 21,079 --.-K/s in 0s2020-06-17 17:42:51 (221 MB/s) - ‘calico-3.13.1.yaml’ saved [21079/21079][root@master55 calico]# lscalico-3.13.1.yaml[root@master55 calico]# 安装calico-3.13.1.yaml script12345678910111213141516171819202122232425[root@master55 calico]# kubectl apply -f calico-3.13.1.yamlconfigmap/calico-config createdcustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org createdclusterrole.rbac.authorization.k8s.io/calico-kube-controllers createdclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers createdclusterrole.rbac.authorization.k8s.io/calico-node createdclusterrolebinding.rbac.authorization.k8s.io/calico-node createddaemonset.apps/calico-node createdserviceaccount/calico-node createddeployment.apps/calico-kube-controllers createdserviceaccount/calico-kube-controllers created[root@master55 calico]# 十六、再次查看node和pod需要等待一会时间 查看nodes，发现状态已经为Ready script123456[root@master55 /]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster55.xincan.cn Ready master 16h v1.18.3slave56.xincan.cn Ready &lt;none&gt; 16h v1.18.3slave57.xincan.cn Ready &lt;none&gt; 16h v1.18.3[root@master55 /]# 查看pods，发现状态都为Running script12345678910111213141516[root@master55 /]# kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system calico-kube-controllers-5b8b769fcd-cbqpr 1/1 Running 0 6m21skube-system calico-node-fnv55 1/1 Running 0 6m21skube-system calico-node-rbpc8 1/1 Running 0 6m21skube-system calico-node-xrsbf 1/1 Running 0 6m21skube-system coredns-7ff77c879f-hz59h 1/1 Running 0 16hkube-system coredns-7ff77c879f-kkdpn 1/1 Running 0 16hkube-system etcd-master55.xincan.cn 1/1 Running 2 16hkube-system kube-apiserver-master55.xincan.cn 1/1 Running 2 16hkube-system kube-controller-manager-master55.xincan.cn 1/1 Running 2 16hkube-system kube-proxy-kdxlv 1/1 Running 2 16hkube-system kube-proxy-mxm5n 1/1 Running 2 16hkube-system kube-proxy-sdnxb 1/1 Running 2 16hkube-system kube-scheduler-master55.xincan.cn 1/1 Running 2 16h[root@master55 /]# 十七、安装gitscript123456789[root@master55 /]# yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker[root@master55 /]# cd /usr/local/src/[root@master55 src]# wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.23.0.tar.xz[root@master55 src]# tar -xvf git-2.23.0.tar.xz[root@master55 src]# cd git-2.23.0/[root@master55 src]# make prefix=/usr/local/git all[root@master55 src]# make prefix=/usr/local/git install[root@master55 src]# echo \"export PATH=$PATH:/usr/local/git/bin\" &gt;&gt; /etc/profile[root@master55 src]# source /etc/profile 十八、kube-promethues部署 下载安装源码，切换到上述创建的k8s目录下，进行git clone https://github.com/coreos/kube-prometheus.git拉取 安装文件都在kube-prometheus/manifests/ 目录下，切换到此目录 script1234567891011121314151617181920212223242526272829[root@master55 k8s]# git clone https://github.com/coreos/kube-prometheus.gitCloning into 'kube-prometheus'...remote: Enumerating objects: 8381, done.remote: Total 8381 (delta 0), reused 0 (delta 0), pack-reused 8381Receiving objects: 100% (8381/8381), 4.70 MiB | 421.00 KiB/s, done.Resolving deltas: 100% (5082/5082), done.[root@master55 k8s]# lscalico kube-prometheus[root@master55 k8s]# cd kube-prometheus/manifests/[root@master55 manifests]# lsalertmanager-alertmanager.yaml node-exporter-clusterRoleBinding.yaml prometheus-clusterRole.yamlalertmanager-secret.yaml node-exporter-clusterRole.yaml prometheus-operator-serviceMonitor.yamlalertmanager-serviceAccount.yaml node-exporter-daemonset.yaml prometheus-prometheus.yamlalertmanager-serviceMonitor.yaml node-exporter-serviceAccount.yaml prometheus-roleBindingConfig.yamlalertmanager-service.yaml node-exporter-serviceMonitor.yaml prometheus-roleBindingSpecificNamespaces.yamlgrafana-dashboardDatasources.yaml node-exporter-service.yaml prometheus-roleConfig.yamlgrafana-dashboardDefinitions.yaml prometheus-adapter-apiService.yaml prometheus-roleSpecificNamespaces.yamlgrafana-dashboardSources.yaml prometheus-adapter-clusterRoleAggregatedMetricsReader.yaml prometheus-rules.yamlgrafana-deployment.yaml prometheus-adapter-clusterRoleBindingDelegator.yaml prometheus-serviceAccount.yamlgrafana-serviceAccount.yaml prometheus-adapter-clusterRoleBinding.yaml prometheus-serviceMonitorApiserver.yamlgrafana-serviceMonitor.yaml prometheus-adapter-clusterRoleServerResources.yaml prometheus-serviceMonitorCoreDNS.yamlgrafana-service.yaml prometheus-adapter-clusterRole.yaml prometheus-serviceMonitorKubeControllerManager.yamlkube-state-metrics-clusterRoleBinding.yaml prometheus-adapter-configMap.yaml prometheus-serviceMonitorKubelet.yamlkube-state-metrics-clusterRole.yaml prometheus-adapter-deployment.yaml prometheus-serviceMonitorKubeScheduler.yamlkube-state-metrics-deployment.yaml prometheus-adapter-roleBindingAuthReader.yaml prometheus-serviceMonitor.yamlkube-state-metrics-serviceAccount.yaml prometheus-adapter-serviceAccount.yaml prometheus-service.yamlkube-state-metrics-serviceMonitor.yaml prometheus-adapter-service.yaml setupkube-state-metrics-service.yaml prometheus-clusterRoleBinding.yaml[root@master55 manifests]# 官方把所有文件都放在一起,这里我复制了然后分类下，切换到k8s目录下进行操作 script1234[root@master55 k8s]# mkdir prometheus[root@master55 k8s]# cp kube-prometheus/manifests/* prometheus/[root@master55 k8s]# cd prometheus/[root@master55 prometheus]# mkdir -p operator node-exporter alertmanager grafana kube-state-metrics prometheus serviceMonitor adapter 将kube-prometheus/manifests/setup下的文件都复制到/prometheus/operator/下 script1[root@master55 k8s]# cp -f kube-prometheus/manifests/setup/* /k8s/prometheus/operator/ 将prometheus/operator/下0namespace-namespace.yaml文件移动到/k8s/prometheus/下 script1[root@master55 k8s]# mv prometheus/operator/0namespace-namespace.yaml prometheus/ 切换到prometheus/目录下，进行文件归置 script12345678[root@master55 k8s]# cd prometheus/[root@master55 prometheus]# mv *-serviceMonitor* serviceMonitor/[root@master55 prometheus]# mv grafana-* grafana/[root@master55 prometheus]# mv kube-state-metrics-* kube-state-metrics/[root@master55 prometheus]# mv alertmanager-* alertmanager/[root@master55 prometheus]# mv node-exporter-* node-exporter/[root@master55 prometheus]# mv prometheus-adapter* adapter/[root@master55 prometheus]# mv prometheus-* prometheus/ 注意：新版本的默认label变了，需要修改选择器为beta.kubernetes.io/os，不然安装的时候会卡住,修改选择器 script123456[root@master55 prometheus]# sed -ri '/linux/s#kubernetes.io#beta.&amp;#' \\ alertmanager/alertmanager-alertmanager.yaml \\ prometheus/prometheus-prometheus.yaml \\ node-exporter/node-exporter-daemonset.yaml \\ kube-state-metrics/kube-state-metrics-deployment.yaml[root@master55 prometheus]# 注意：镜像使用dockerhub上的 script12345[root@master55 prometheus]# sed -ri '/quay.io/s#quay.io/prometheus#prom#' \\ alertmanager/alertmanager-alertmanager.yaml \\ prometheus/prometheus-prometheus.yaml \\ node-exporter/node-exporter-daemonset.yaml[root@master55 prometheus]# 注意：镜像使用dockerhub上的 script12[root@master55 prometheus]# find -type f -exec sed -ri 's#k8s.gcr.io#gcr.azk8s.cn/google_containers#' {} \\; [root@master55 prometheus]# 生成namespace script123[root@master55 prometheus]# kubectl apply -f .namespace/monitoring created[root@master55 prometheus]# 安装operater script12345678910111213[root@master55 prometheus]# kubectl apply -f operator/customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com createdcustomresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com createdcustomresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com createdcustomresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com createdcustomresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com createdcustomresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com createdclusterrole.rbac.authorization.k8s.io/prometheus-operator createdclusterrolebinding.rbac.authorization.k8s.io/prometheus-operator createddeployment.apps/prometheus-operator createdservice/prometheus-operator createdserviceaccount/prometheus-operator created[root@master55 prometheus]# 依次安装其他组件 script12345678[root@master55 prometheus]# kubectl apply -f adapter/[root@master55 prometheus]# kubectl apply -f alertmanager/[root@master55 prometheus]# kubectl apply -f node-exporter/[root@master55 prometheus]# kubectl apply -f kube-state-metrics/[root@master55 prometheus]# kubectl apply -f grafana/[root@master55 prometheus]# kubectl apply -f prometheus/[root@master55 prometheus]# kubectl apply -f serviceMonitor/[root@master55 prometheus]# 查看整体状态 script1234567891011121314151617181920212223242526272829303132333435363738394041[root@master55 /]# kubectl -n monitoring get allNAME READY STATUS RESTARTS AGEpod/alertmanager-main-0 2/2 Running 0 28mpod/alertmanager-main-1 2/2 Running 0 28mpod/alertmanager-main-2 2/2 Running 0 28mpod/grafana-5c55845445-hzhbk 1/1 Running 0 38mpod/kube-state-metrics-665c856fb9-7ggrg 3/3 Running 0 38mpod/node-exporter-dqfd7 2/2 Running 0 39mpod/node-exporter-gf8gr 2/2 Running 0 39mpod/node-exporter-mcl79 2/2 Running 0 39mpod/prometheus-adapter-5cdcdf9c8d-665fj 1/1 Running 0 39mpod/prometheus-operator-6f98f66b89-6spkk 2/2 Running 0 32mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/alertmanager-main ClusterIP 10.111.117.158 &lt;none&gt; 9093/TCP 39mservice/alertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,9094/TCP,9094/UDP 28mservice/grafana ClusterIP 10.111.25.63 &lt;none&gt; 3000/TCP 38mservice/kube-state-metrics ClusterIP None &lt;none&gt; 8443/TCP,9443/TCP 38mservice/node-exporter ClusterIP None &lt;none&gt; 9100/TCP 39mservice/prometheus-adapter ClusterIP 10.109.99.195 &lt;none&gt; 443/TCP 39mservice/prometheus-k8s ClusterIP 10.110.243.126 &lt;none&gt; 9090/TCP 38mservice/prometheus-operator ClusterIP None &lt;none&gt; 8443/TCP 32mNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdaemonset.apps/node-exporter 3 3 3 3 3 beta.kubernetes.io/os=linux 39mNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/grafana 1/1 1 1 38mdeployment.apps/kube-state-metrics 1/1 1 1 38mdeployment.apps/prometheus-adapter 1/1 1 1 39mdeployment.apps/prometheus-operator 1/1 1 1 32mNAME DESIRED CURRENT READY AGEreplicaset.apps/grafana-5c55845445 1 1 1 38mreplicaset.apps/kube-state-metrics-665c856fb9 1 1 1 38mreplicaset.apps/prometheus-adapter-5cdcdf9c8d 1 1 1 39mreplicaset.apps/prometheus-operator-6f98f66b89 1 1 1 32mNAME READY AGEstatefulset.apps/alertmanager-main 3/3 28m[root@master55 /]# k8s查看所有svc script1234567891011[root@master55 prometheus]# kubectl -n monitoring get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEalertmanager-main ClusterIP 10.111.117.158 &lt;none&gt; 9093/TCP 115malertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,9094/TCP,9094/UDP 104mgrafana NodePort 10.111.25.63 &lt;none&gt; 3000:31533/TCP 114mkube-state-metrics ClusterIP None &lt;none&gt; 8443/TCP,9443/TCP 115mnode-exporter ClusterIP None &lt;none&gt; 9100/TCP 115mprometheus-adapter ClusterIP 10.109.99.195 &lt;none&gt; 443/TCP 115mprometheus-k8s ClusterIP 10.110.243.126 &lt;none&gt; 9090/TCP 114mprometheus-operator ClusterIP None &lt;none&gt; 8443/TCP 108m[root@master55 prometheus]# k8s暴露grafana外网访问端口 修改type: ClusterIP为 type: NodePort ,默认外网端口NodePort对应的是31533，找到如下代码， script123456789101112131415[root@master55 prometheus]# kubectl -n monitoring edit svc grafanaspec: clusterIP: 10.111.25.63 externalTrafficPolicy: Cluster ports: - name: http nodePort: 31533 port: 3000 protocol: TCP targetPort: http selector: app: grafana sessionAffinity: None type: NodePort 最终效果图（部分效果图） 十九、 安装kubernetes-dashboard下载dashboard所需资源 git clone https://github.com/xincan/kubernetes.git script12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@xincan /]#cd /k8s/[root@xincan k8s]#git clone https://github.com/xincan/kubernetes.git[root@xincan k8s]#lscalico kube-prometheus kubernetes-dashboard prometheus[root@master55 k8s]# cd kubernetes-dashboard/[root@master55 kubernetes-dashboard]# lslogin-token recommended.yaml[root@master55 kubernetes-dashboard]#kubectl create -f recommended.yamlnamespace/kubernetes-dashboard createdserviceaccount/kubernetes-dashboard createdservice/kubernetes-dashboard createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-csrf createdsecret/kubernetes-dashboard-key-holder createdconfigmap/kubernetes-dashboard-settings createdrole.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/dashboard-metrics-scraper createddeployment.apps/dashboard-metrics-scraper created[root@master55 kubernetes-dashboard]### 查看kubernetes-dashboard命名空间下pod，svc[root@master55 kubernetes-dashboard]# kubectl get pod,svc -n kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/dashboard-metrics-scraper-779f5454cb-hzgc4 1/1 ContainerCreating 0 30spod/kubernetes-dashboard-857bb4c778-gsf2q 1/1 ContainerCreating 0 30sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/dashboard-metrics-scraper ClusterIP 10.105.54.169 &lt;none&gt; 8000/TCP 30sservice/kubernetes-dashboard ClusterIP 10.110.40.170 &lt;none&gt; 443/TCP 31s[root@master55 kubernetes-dashboard]### 增加nodePort: 30001,设置dashboard访问端口，修改type: NodePort,保存退出自动更新## 也可以使用kubectl patch svc -n kube-system kubernetes-dashboard -p '{\"spec\":{\"type\":\"NodePort\"}}'进行修改[root@master55 kubernetes-dashboard]#kubectl edit svc -n kubernetes-dashboard kubernetes-dashboardspec: clusterIP: 10.110.40.170 externalTrafficPolicy: Cluster ports: - nodePort: 30000 port: 443 protocol: TCP targetPort: 8443 selector: k8s-app: kubernetes-dashboard sessionAffinity: None type: NodePortdit cancelled, no changes made.## 修改完成后，可以看到kubernetes-dashboard的type为NodePort，端口为30000[root@master55 kubernetes-dashboard]# kubectl get svc -n kubernetes-dashboard kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.110.40.170 &lt;none&gt; 443:30001/TCP 14m[root@master55 kubernetes-dashboard]# 二十、 Token令牌登录script1234567891011121314151617181920212223242526272829303132333435363738394041424344## 在kube-system命名空间下创建xincan-dashboard-admin用户[root@master55 k8s]# kubectl create serviceaccount xincan-dashboard-admin -n kube-systemserviceaccount/xincan-dashboard-admin created## 查看用户[root@master55 k8s]# kubectl get serviceaccount xincan-dashboard-admin -n kube-systemNAME SECRETS AGExincan-dashboard-admin 1 7s[root@master55 k8s]### 在kube-system命名空间下，创建xincan-dashboard-admin权限[root@master55 k8s]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:xincan-dashboard-adminclusterrolebinding.rbac.authorization.k8s.io/dashboard-cluster-admin created## 查看权限xincan-dashboard-admin是否生效[root@master55 k8s]# kubectl get secret -n kube-systemNAME TYPE DATA AGEattachdetach-controller-token-fk6c9 kubernetes.io/service-account-token 3 91mbootstrap-signer-token-kjw62 kubernetes.io/service-account-token 3 91mbootstrap-token-90s06v bootstrap.kubernetes.io/token 7 91mcalico-kube-controllers-token-6824q kubernetes.io/service-account-token 3 85mcalico-node-token-22rkl kubernetes.io/service-account-token 3 85mcertificate-controller-token-4zhbc kubernetes.io/service-account-token 3 91mclusterrole-aggregation-controller-token-vzbqs kubernetes.io/service-account-token 3 91mcoredns-token-xrqs4 kubernetes.io/service-account-token 3 91mxincan-dashboard-admin-token-jkhl2 kubernetes.io/service-account-token 3 3m35s[root@master55 k8s]### 获取xincan-dashboard-admin-token-jkhl2的token令牌[root@master55 k8s]# kubectl describe secret -n kube-system xincan-dashboard-admin-token-jkhl2Name: xincan-dashboard-admin-token-jkhl2Namespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: xincan-dashboard-admin kubernetes.io/service-account.uid: c257c2f8-57cf-41d7-b8e6-b833a8ef0790Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IkFfZDQ5YTBkcTIwVG1xdF9rWFJxWDJfblFMd1lfdWQwdllVVjFxZTVtcTQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ4aW5jYW4tZGFzaGJvYXJkLWFkbWluLXRva2VuLWpraGwyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InhpbmNhbi1kYXNoYm9hcmQtYWRtaW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjMjU3YzJmOC01N2NmLTQxZDctYjhlNi1iODMzYThlZjA3OTAiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06eGluY2FuLWRhc2hib2FyZC1hZG1pbiJ9.rPDqRpuVrQmTXJ7FHKz8isG4IqYs7aN5QgZCLVRLK1Ul0677pmPfFTgZpVzf5R9f6_h7kFLGFej9wq5QJ3NZn71jv1qhn9rYgabyZ9KsDZsE6SDwQQuHcVDz7iQ9prFezyqdiyBIxqoFEFhZyZe8pn-Ua53a7-P4Dm2xs2xMgbvrLt6b_b8--H_plV-6xrLKrM5BhG15HDi5MA7MXBZJzxTyuNC8CtjQ6ShuQFv5I3Fwqgugu9tqxGBk9Xjy82JGrdnvoSNRNThCMzlVZmClzrsT6CZ4BUw4t0x4dYhvbdo6IS5nnW0u_EOsaDdi1gqBjpeMX_tNT1ChqN55TnnPPw[root@master55 k8s]### 复制令牌然后到登录界面进行粘贴 二十、 KubeConfig登录script1234567891011121314151617181920[root@master55 k8s]# DASH_TOCKEN=$(kubectl get secret -n kube-system xincan-dashboard-admin-token-jkhl2 -o jsonpath={.data.token}|base64 -d)[root@master55 k8s]# kubectl config set-cluster kubernetes --server=https://kubernetes.docker.internal:6443 --kubeconfig=/k8s/xincan-dashbord-admin.confCluster \"kubernetes\" set.[root@master55 k8s]# kubectl config set-credentials xincan-dashboard-admin --token=$DASH_TOCKEN --kubeconfig=/k8s/xincan-dashbord-admin.confUser \"xincan-dashboard-admin\" set.[root@master55 k8s]# kubectl config set-context dashboard-admin@kubernetes --cluster=kubernetes --user=xincan-dashboard-admin --kubeconfig=/k8s/xincan-dashbord-admin.confContext \"dashboard-admin@kubernetes\" created.[root@master55 k8s]# kubectl config use-context dashboard-admin@kubernetes --kubeconfig=/k8s/xincan-dashbord-admin.confSwitched to context \"dashboard-admin@kubernetes\".[root@master55 k8s]# lltotal 710420-rw-r--r-- 1 root root 21079 Jul 12 22:24 calico-3.13.1.yaml-rwxr-xr-x 1 root root 2051 Jul 16 14:40 get-k8s-images.sh-rw-r--r-- 1 root root 727422976 Jul 16 14:57 k8s-imagesV1.18.5.tar-rw-r--r-- 1 root root 901 Jul 16 15:54 k8s-tokendrwxr-xr-x 3 root root 4096 Jul 16 17:47 kube-prometheusdrwxr-xr-x 2 root root 30 Jul 16 17:47 kubernetes-dashboard-rw------- 1 root root 1321 Jul 16 17:01 xincan-dashbord-admin.conf[root@master55 k8s]#","link":"/posts/2aa23666/"},{"title":"centos单机基础环境构建","text":"虚拟机配置 准备一台虚拟机,本教程是以vagrant+vbox进行构建，默认用户名密码均为vagrant,下面开始设置系统，大家也可以自行安装系统。 一、 关闭防火墙、并设置开机不启动防火墙script12sudo systemctl stop firewalldsudo systemctl disable firewalld 二、 开启远程访问script12sudo sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_configsudo systemctl restart sshd 三、 永久关闭selinux修改/etc/sysconfig/selinux文件设置 script123sudo sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinuxsudo sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinuxsudo sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config 四、 禁用swap交换分区打开/etc/fstab注释掉swap script123sudo sed -i 's/.*swap.*/#&amp;/' /etc/fstabsudo swapoff -a &amp;&amp; swapon -asudo sysctl -p 五、 安装wget提示Complete!则安装成功 script123456789101112131415161718192021222324252627282930313233343536373839[root@localhost ~]# sudo yum install -y wgetLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.huaweicloud.com * extras: mirrors.huaweicloud.com * updates: mirrors.ustc.edu.cnResolving Dependencies--&gt; Running transaction check---&gt; Package wget.x86_64 0:1.14-18.el7_6.1 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved=================================================================================================================================================================================================================== Package Arch Version Repository Size===================================================================================================================================================================================================================Installing: wget x86_64 1.14-18.el7_6.1 base 547 kTransaction Summary===================================================================================================================================================================================================================Install 1 PackageTotal download size: 547 kInstalled size: 2.0 MDownloading packages:wget-1.14-18.el7_6.1.x86_64.rpm | 547 kB 00:00:04Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : wget-1.14-18.el7_6.1.x86_64 1/1 Verifying : wget-1.14-18.el7_6.1.x86_64 1/1Installed: wget.x86_64 0:1.14-18.el7_6.1Complete![root@localhost ~]# 六、 更改系统默认镜像源为阿里镜像源 切换到/etc/yum.repos.d/下查看当前镜像源 script1234567891011[root@localhost ~]# cd /etc/yum.repos.d/[root@localhost yum.repos.d]# lltotal 32-rw-r--r--. 1 root root 1664 Nov 23 2018 CentOS-Base.repo-rw-r--r--. 1 root root 1309 Nov 23 2018 CentOS-CR.repo-rw-r--r--. 1 root root 649 Nov 23 2018 CentOS-Debuginfo.repo-rw-r--r--. 1 root root 314 Nov 23 2018 CentOS-fasttrack.repo-rw-r--r--. 1 root root 630 Nov 23 2018 CentOS-Media.repo-rw-r--r--. 1 root root 1331 Nov 23 2018 CentOS-Sources.repo-rw-r--r--. 1 root root 5701 Nov 23 2018 CentOS-Vault.repo[root@localhost ~]# 切换到/etc/yum.repos.d/目录下创建backup文件夹，并将上述镜像源移动到backup文件夹下 script12345[root@localhost yum.repos.d]# sudo mkdir /etc/yum.repos.d/backup &amp;&amp; mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup[root@localhost yum.repos.d]# lltotal 0drwxr-xr-x. 2 root root 187 Jun 4 04:48 backup[root@localhost yum.repos.d]# 拉取阿里云镜像源，看见saved则拉取成功 script123456789101112[root@localhost yum.repos.d]# sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo--2020-06-04 04:48:58-- http://mirrors.aliyun.com/repo/Centos-7.repoResolving mirrors.aliyun.com (mirrors.aliyun.com)... 124.200.113.116, 124.200.113.115, 219.238.20.87, ...Connecting to mirrors.aliyun.com (mirrors.aliyun.com)|124.200.113.116|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 2523 (2.5K) [application/octet-stream]Saving to: '/etc/yum.repos.d/CentOS-Base.repo'100%[=========================================================================================================================================================================&gt;] 2,523 --.-K/s in 0s2020-06-04 04:48:58 (264 MB/s) - '/etc/yum.repos.d/CentOS-Base.repo' saved [2523/2523][root@localhost yum.repos.d]# 查看目录结构 script12345[root@localhost yum.repos.d]# lltotal 4drwxr-xr-x. 2 root root 187 Jun 4 04:48 backup-rw-r--r--. 1 root root 2523 Jun 15 2018 CentOS-Base.repo[root@localhost yum.repos.d]# 执行yum clean all清空镜像源缓存 script12345[root@localhost yum.repos.d]# sudo yum clean allLoaded plugins: fastestmirrorCleaning repos: base extras updatesCleaning up list of fastest mirrors[root@localhost yum.repos.d]# 执行yum makecache重新加载缓存，看到Metadata Cache Created则加载成功 script123456789101112131415161718192021[root@localhost yum.repos.d]# sudo yum makecacheLoaded plugins: fastestmirrorDetermining fastest mirrors * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.combase | 3.6 kB 00:00:00extras | 2.9 kB 00:00:00updates | 2.9 kB 00:00:00(1/10): base/7/x86_64/group_gz | 153 kB 00:00:00(2/10): extras/7/x86_64/filelists_db | 205 kB 00:00:00(3/10): extras/7/x86_64/other_db | 122 kB 00:00:00(4/10): base/7/x86_64/other_db | 2.6 MB 00:00:00(5/10): updates/7/x86_64/primary_db | 1.3 MB 00:00:00(6/10): updates/7/x86_64/filelists_db | 997 kB 00:00:00(7/10): updates/7/x86_64/other_db | 192 kB 00:00:00(8/10): base/7/x86_64/primary_db | 6.1 MB 00:00:01(9/10): extras/7/x86_64/primary_db | 194 kB 00:00:00(10/10): base/7/x86_64/filelists_db | 7.1 MB 00:00:02Metadata Cache Created[root@localhost yum.repos.d]# 七、 查看服务器IP地址（非必须）我个人的是在eth1下，对应的是192.168.1.40 script1234567891011121314151617181920[root@localhost ~]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:00:8a:fe:e6 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic eth0 valid_lft 77402sec preferred_lft 77402sec inet6 fe80::5054:ff:fe8a:fee6/64 scope link valid_lft forever preferred_lft forever3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 08:00:27:dd:2a:77 brd ff:ff:ff:ff:ff:ff inet 192.168.1.40/24 brd 192.168.1.255 scope global noprefixroute eth1 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fedd:2a77/64 scope link valid_lft forever preferred_lft forever[root@localhost ~]# 八、 自定义服务器域名（非必须）也可以手动修改vi /etc/hostname(需要重启) script1234[root@localhost ~]# hostname xincan.base.com[root@localhost ~]# hostnamexincan.base.com[root@localhost ~]# 九、 添加hosts自定义域名映射（非必须） 修改hosts配置 script12345[root@localhost ~]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.40 xincan.base.com [root@localhost ~]# 查看hosts配置 script12345[root@localhost ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.40 xincan.base.com[root@localhost ~]# 十、 同步阿里云时间（墙裂建议） 安装ntp，提示Complete!则安装成功 script12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@localhost ~]# sudo yum install -y ntpLoaded plugins: fastestmirrorDetermining fastest mirrors * base: mirrors.huaweicloud.com * extras: mirrors.huaweicloud.com * updates: mirrors.ustc.edu.cnbase | 3.6 kB 00:00:00extras | 2.9 kB 00:00:00updates | 2.9 kB 00:00:00(1/4): base/7/x86_64/group_gz | 153 kB 00:00:00(2/4): extras/7/x86_64/primary_db | 194 kB 00:00:00(3/4): updates/7/x86_64/primary_db | 2.1 MB 00:00:01(4/4): base/7/x86_64/primary_db | 6.1 MB 00:00:05Resolving Dependencies--&gt; Running transaction check---&gt; Package ntp.x86_64 0:4.2.6p5-29.el7.centos will be installed--&gt; Processing Dependency: ntpdate = 4.2.6p5-29.el7.centos for package: ntp-4.2.6p5-29.el7.centos.x86_64--&gt; Processing Dependency: libopts.so.25()(64bit) for package: ntp-4.2.6p5-29.el7.centos.x86_64--&gt; Running transaction check---&gt; Package autogen-libopts.x86_64 0:5.18-5.el7 will be installed---&gt; Package ntpdate.x86_64 0:4.2.6p5-29.el7.centos will be installed--&gt; Finished Dependency ResolutionDependencies Resolved===================================================================================================================================================== Package Arch Version Repository Size=====================================================================================================================================================Installing: ntp x86_64 4.2.6p5-29.el7.centos base 548 kInstalling for dependencies: autogen-libopts x86_64 5.18-5.el7 base 66 k ntpdate x86_64 4.2.6p5-29.el7.centos base 86 kTransaction Summary=====================================================================================================================================================Install 1 Package (+2 Dependent packages)Total download size: 701 kInstalled size: 1.6 MDownloading packages:warning: /var/cache/yum/x86_64/7/base/packages/ntpdate-4.2.6p5-29.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY- ETAPublic key for ntpdate-4.2.6p5-29.el7.centos.x86_64.rpm is not installed(1/3): ntpdate-4.2.6p5-29.el7.centos.x86_64.rpm | 86 kB 00:00:00(2/3): autogen-libopts-5.18-5.el7.x86_64.rpm | 66 kB 00:00:00(3/3): ntp-4.2.6p5-29.el7.centos.x86_64.rpm | 548 kB 00:00:00-----------------------------------------------------------------------------------------------------------------------------------------------------Total 956 kB/s | 701 kB 00:00:00Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7Importing GPG key 0xF4A80EB5: Userid : \"CentOS-7 Key (CentOS 7 Official Signing Key) &lt;security@centos.org&gt;\" Fingerprint: 6341 ab27 53d7 8a78 a7c2 7bb1 24c6 a8a7 f4a8 0eb5 Package : centos-release-7-6.1810.2.el7.centos.x86_64 (@anaconda) From : /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : autogen-libopts-5.18-5.el7.x86_64 1/3 Installing : ntpdate-4.2.6p5-29.el7.centos.x86_64 2/3 Installing : ntp-4.2.6p5-29.el7.centos.x86_64 3/3 Verifying : ntp-4.2.6p5-29.el7.centos.x86_64 1/3 Verifying : ntpdate-4.2.6p5-29.el7.centos.x86_64 2/3 Verifying : autogen-libopts-5.18-5.el7.x86_64 3/3Installed: ntp.x86_64 0:4.2.6p5-29.el7.centosDependency Installed: autogen-libopts.x86_64 0:5.18-5.el7 ntpdate.x86_64 0:4.2.6p5-29.el7.centosComplete![root@localhost ~]# 查看当前系统时间、并设置当前时间为上海 script123456[root@localhost /]# dateThu Jun 4 05:28:48 UTC 2020[root@localhost /]# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime[root@localhost /]# dateThu Jun 4 13:29:01 CST 2020[root@localhost /]# 设置ntp，同步阿里云时间，执行vi /etc/ntp.conf找到server四行代码注释掉，在其下面增加server aliyun.com iburst，然后通过sudo systemctl start ntpd启动服务，稍等一会执行ntpq -p查看是否同步，如果出现前面的*则，同步成功 systemctl start ntpd &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;启动ntp systemctl restart ntpd &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;重启ntp systemctl enable ntpd.service &nbsp;&nbsp;开机启动 ntpdc -c loopinfo &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;查看与时间同步服务器的时间差 script123456789101112131415161718[root@localhost /]# vi /etc/ntp.conf# server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburstserver ntp.aliyun.com iburst[root@localhost /]#[root@localhost /]# sudo systemctl start ntpd[root@localhost /]# systemctl enable ntpd.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.[root@localhost /]# ntpq -p remote refid st t when poll reach delay offset jitter==============================================================================*203.107.6.88 10.137.38.86 2 u 37 64 1 53.476 -5.668 2.224[root@localhost /]#","link":"/posts/d05af695/"},{"title":"centos构建docker基础环境","text":"环境准备准备一台虚拟机，请参照centos基础环境构建 一、 检查是否之前安装过docker，如果有则清除docker环境script123456789101112[root@localhost /]# sudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engineLoaded plugins: fastestmirrorNo Match for argument: dockerNo Match for argument: docker-clientNo Match for argument: docker-client-latestNo Match for argument: docker-commonNo Match for argument: docker-latestNo Match for argument: docker-latest-logrotateNo Match for argument: docker-logrotateNo Match for argument: docker-engineNo Packages marked for removal[root@localhost /]# 二、 安装yum、docker环境依赖见到Complete!，则安装成功 script123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118[root@localhost /]# sudo yum install -y yum-utils device-mapper-persistent-data lvm2Loaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.comResolving Dependencies--&gt; Running transaction check---&gt; Package device-mapper-persistent-data.x86_64 0:0.8.5-2.el7 will be installed--&gt; Processing Dependency: libaio.so.1(LIBAIO_0.4)(64bit) for package: device-mapper-persistent-data-0.8.5-2.el7.x86_64--&gt; Processing Dependency: libaio.so.1(LIBAIO_0.1)(64bit) for package: device-mapper-persistent-data-0.8.5-2.el7.x86_64--&gt; Processing Dependency: libaio.so.1()(64bit) for package: device-mapper-persistent-data-0.8.5-2.el7.x86_64---&gt; Package lvm2.x86_64 7:2.02.186-7.el7_8.2 will be installed--&gt; Processing Dependency: lvm2-libs = 7:2.02.186-7.el7_8.2 for package: 7:lvm2-2.02.186-7.el7_8.2.x86_64--&gt; Processing Dependency: liblvm2app.so.2.2(Base)(64bit) for package: 7:lvm2-2.02.186-7.el7_8.2.x86_64--&gt; Processing Dependency: libdevmapper-event.so.1.02(Base)(64bit) for package: 7:lvm2-2.02.186-7.el7_8.2.x86_64--&gt; Processing Dependency: liblvm2app.so.2.2()(64bit) for package: 7:lvm2-2.02.186-7.el7_8.2.x86_64--&gt; Processing Dependency: libdevmapper-event.so.1.02()(64bit) for package: 7:lvm2-2.02.186-7.el7_8.2.x86_64---&gt; Package yum-utils.noarch 0:1.1.31-50.el7 will be updated---&gt; Package yum-utils.noarch 0:1.1.31-54.el7_8 will be an update--&gt; Running transaction check---&gt; Package device-mapper-event-libs.x86_64 7:1.02.164-7.el7_8.2 will be installed---&gt; Package libaio.x86_64 0:0.3.109-13.el7 will be installed---&gt; Package lvm2-libs.x86_64 7:2.02.186-7.el7_8.2 will be installed--&gt; Processing Dependency: device-mapper-event = 7:1.02.164-7.el7_8.2 for package: 7:lvm2-libs-2.02.186-7.el7_8.2.x86_64--&gt; Running transaction check---&gt; Package device-mapper-event.x86_64 7:1.02.164-7.el7_8.2 will be installed--&gt; Processing Dependency: device-mapper = 7:1.02.164-7.el7_8.2 for package: 7:device-mapper-event-1.02.164-7.el7_8.2.x86_64--&gt; Running transaction check---&gt; Package device-mapper.x86_64 7:1.02.149-10.el7_6.7 will be updated--&gt; Processing Dependency: device-mapper = 7:1.02.149-10.el7_6.7 for package: 7:device-mapper-libs-1.02.149-10.el7_6.7.x86_64---&gt; Package device-mapper.x86_64 7:1.02.164-7.el7_8.2 will be an update--&gt; Running transaction check---&gt; Package device-mapper-libs.x86_64 7:1.02.149-10.el7_6.7 will be updated---&gt; Package device-mapper-libs.x86_64 7:1.02.164-7.el7_8.2 will be an update--&gt; Finished Dependency ResolutionDependencies Resolved=================================================================================================================================================================================================================== Package Arch Version Repository Size===================================================================================================================================================================================================================Installing: device-mapper-persistent-data x86_64 0.8.5-2.el7 base 422 k lvm2 x86_64 7:2.02.186-7.el7_8.2 updates 1.3 MUpdating: yum-utils noarch 1.1.31-54.el7_8 updates 122 kInstalling for dependencies: device-mapper-event x86_64 7:1.02.164-7.el7_8.2 updates 191 k device-mapper-event-libs x86_64 7:1.02.164-7.el7_8.2 updates 190 k libaio x86_64 0.3.109-13.el7 base 24 k lvm2-libs x86_64 7:2.02.186-7.el7_8.2 updates 1.1 MUpdating for dependencies: device-mapper x86_64 7:1.02.164-7.el7_8.2 updates 295 k device-mapper-libs x86_64 7:1.02.164-7.el7_8.2 updates 324 kTransaction Summary===================================================================================================================================================================================================================Install 2 Packages (+4 Dependent packages)Upgrade 1 Package (+2 Dependent packages)Total download size: 3.9 MDownloading packages:No Presto metadata available for updates(1/9): device-mapper-1.02.164-7.el7_8.2.x86_64.rpm | 295 kB 00:00:00(2/9): device-mapper-event-1.02.164-7.el7_8.2.x86_64.rpm | 191 kB 00:00:00(3/9): device-mapper-event-libs-1.02.164-7.el7_8.2.x86_64.rpm | 190 kB 00:00:00(4/9): device-mapper-libs-1.02.164-7.el7_8.2.x86_64.rpm | 324 kB 00:00:00(5/9): libaio-0.3.109-13.el7.x86_64.rpm | 24 kB 00:00:00(6/9): device-mapper-persistent-data-0.8.5-2.el7.x86_64.rpm | 422 kB 00:00:00(7/9): lvm2-2.02.186-7.el7_8.2.x86_64.rpm | 1.3 MB 00:00:01(8/9): lvm2-libs-2.02.186-7.el7_8.2.x86_64.rpm | 1.1 MB 00:00:00(9/9): yum-utils-1.1.31-54.el7_8.noarch.rpm | 122 kB 00:00:00-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Total 2.3 MB/s | 3.9 MB 00:00:01Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : libaio-0.3.109-13.el7.x86_64 1/12 Updating : 7:device-mapper-libs-1.02.164-7.el7_8.2.x86_64 2/12 Updating : 7:device-mapper-1.02.164-7.el7_8.2.x86_64 3/12 Installing : 7:device-mapper-event-libs-1.02.164-7.el7_8.2.x86_64 4/12 Installing : 7:device-mapper-event-1.02.164-7.el7_8.2.x86_64 5/12 Installing : 7:lvm2-libs-2.02.186-7.el7_8.2.x86_64 6/12 Installing : device-mapper-persistent-data-0.8.5-2.el7.x86_64 7/12 Installing : 7:lvm2-2.02.186-7.el7_8.2.x86_64 8/12 Updating : yum-utils-1.1.31-54.el7_8.noarch 9/12 Cleanup : yum-utils-1.1.31-50.el7.noarch 10/12 Cleanup : 7:device-mapper-libs-1.02.149-10.el7_6.7.x86_64 11/12 Cleanup : 7:device-mapper-1.02.149-10.el7_6.7.x86_64 12/12 Verifying : 7:device-mapper-1.02.164-7.el7_8.2.x86_64 1/12 Verifying : device-mapper-persistent-data-0.8.5-2.el7.x86_64 2/12 Verifying : 7:lvm2-libs-2.02.186-7.el7_8.2.x86_64 3/12 Verifying : 7:device-mapper-event-1.02.164-7.el7_8.2.x86_64 4/12 Verifying : 7:lvm2-2.02.186-7.el7_8.2.x86_64 5/12 Verifying : 7:device-mapper-libs-1.02.164-7.el7_8.2.x86_64 6/12 Verifying : libaio-0.3.109-13.el7.x86_64 7/12 Verifying : yum-utils-1.1.31-54.el7_8.noarch 8/12 Verifying : 7:device-mapper-event-libs-1.02.164-7.el7_8.2.x86_64 9/12 Verifying : 7:device-mapper-libs-1.02.149-10.el7_6.7.x86_64 10/12 Verifying : yum-utils-1.1.31-50.el7.noarch 11/12 Verifying : 7:device-mapper-1.02.149-10.el7_6.7.x86_64 12/12Installed: device-mapper-persistent-data.x86_64 0:0.8.5-2.el7 lvm2.x86_64 7:2.02.186-7.el7_8.2Dependency Installed: device-mapper-event.x86_64 7:1.02.164-7.el7_8.2 device-mapper-event-libs.x86_64 7:1.02.164-7.el7_8.2 libaio.x86_64 0:0.3.109-13.el7 lvm2-libs.x86_64 7:2.02.186-7.el7_8.2Updated: yum-utils.noarch 0:1.1.31-54.el7_8Dependency Updated: device-mapper.x86_64 7:1.02.164-7.el7_8.2 device-mapper-libs.x86_64 7:1.02.164-7.el7_8.2Complete![root@localhost /]# 三、 配置docker源script12345678910[root@localhost /]# sudo yum-config-manager -y --add-repo https://download.docker.com/linux/centos/docker-ce.repoLoaded plugins: fastestmirroradding repo from: https://download.docker.com/linux/centos/docker-ce.repograbbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.reporepo saved to /etc/yum.repos.d/docker-ce.repo[root@localhost /]#[root@localhost /]# cd /etc/yum.repos.d/[root@localhost yum.repos.d]# lsbackup CentOS-Base.repo docker-ce.repo[root@localhost yum.repos.d]# 四、 安装docker-ce-cli客户端、docker-ce服务端见到Complete!，则安装成功 script123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156[root@localhost /]# sudo yum install -y docker-ce docker-ce-cli containerd.ioLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.comdocker-ce-stable | 3.5 kB 00:00:00(1/2): docker-ce-stable/x86_64/updateinfo | 55 B 00:00:00(2/2): docker-ce-stable/x86_64/primary_db | 44 kB 00:00:01Resolving Dependencies--&gt; Running transaction check---&gt; Package containerd.io.x86_64 0:1.2.13-3.2.el7 will be installed--&gt; Processing Dependency: container-selinux &gt;= 2:2.74 for package: containerd.io-1.2.13-3.2.el7.x86_64---&gt; Package docker-ce.x86_64 3:19.03.11-3.el7 will be installed--&gt; Processing Dependency: libcgroup for package: 3:docker-ce-19.03.11-3.el7.x86_64---&gt; Package docker-ce-cli.x86_64 1:19.03.11-3.el7 will be installed--&gt; Running transaction check---&gt; Package container-selinux.noarch 2:2.119.1-1.c57a6f9.el7 will be installed--&gt; Processing Dependency: policycoreutils-python for package: 2:container-selinux-2.119.1-1.c57a6f9.el7.noarch---&gt; Package libcgroup.x86_64 0:0.41-21.el7 will be installed--&gt; Running transaction check---&gt; Package policycoreutils-python.x86_64 0:2.5-34.el7 will be installed--&gt; Processing Dependency: policycoreutils = 2.5-34.el7 for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: setools-libs &gt;= 3.3.8-4 for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: libsemanage-python &gt;= 2.5-14 for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: audit-libs-python &gt;= 2.1.3-4 for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: python-IPy for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: libqpol.so.1(VERS_1.4)(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: libqpol.so.1(VERS_1.2)(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: libapol.so.4(VERS_4.0)(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: checkpolicy for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: libqpol.so.1()(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Processing Dependency: libapol.so.4()(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64--&gt; Running transaction check---&gt; Package audit-libs-python.x86_64 0:2.8.5-4.el7 will be installed--&gt; Processing Dependency: audit-libs(x86-64) = 2.8.5-4.el7 for package: audit-libs-python-2.8.5-4.el7.x86_64---&gt; Package checkpolicy.x86_64 0:2.5-8.el7 will be installed---&gt; Package libsemanage-python.x86_64 0:2.5-14.el7 will be installed---&gt; Package policycoreutils.x86_64 0:2.5-29.el7_6.1 will be updated---&gt; Package policycoreutils.x86_64 0:2.5-34.el7 will be an update---&gt; Package python-IPy.noarch 0:0.75-6.el7 will be installed---&gt; Package setools-libs.x86_64 0:3.3.8-4.el7 will be installed--&gt; Running transaction check---&gt; Package audit-libs.x86_64 0:2.8.4-4.el7 will be updated--&gt; Processing Dependency: audit-libs(x86-64) = 2.8.4-4.el7 for package: audit-2.8.4-4.el7.x86_64---&gt; Package audit-libs.x86_64 0:2.8.5-4.el7 will be an update--&gt; Running transaction check---&gt; Package audit.x86_64 0:2.8.4-4.el7 will be updated---&gt; Package audit.x86_64 0:2.8.5-4.el7 will be an update--&gt; Finished Dependency ResolutionDependencies Resolved=================================================================================================================================================================================================================== Package Arch Version Repository Size===================================================================================================================================================================================================================Installing: containerd.io x86_64 1.2.13-3.2.el7 docker-ce-stable 25 M docker-ce x86_64 3:19.03.11-3.el7 docker-ce-stable 24 M docker-ce-cli x86_64 1:19.03.11-3.el7 docker-ce-stable 38 MInstalling for dependencies: audit-libs-python x86_64 2.8.5-4.el7 base 76 k checkpolicy x86_64 2.5-8.el7 base 295 k container-selinux noarch 2:2.119.1-1.c57a6f9.el7 extras 40 k libcgroup x86_64 0.41-21.el7 base 66 k libsemanage-python x86_64 2.5-14.el7 base 113 k policycoreutils-python x86_64 2.5-34.el7 base 457 k python-IPy noarch 0.75-6.el7 base 32 k setools-libs x86_64 3.3.8-4.el7 base 620 kUpdating for dependencies: audit x86_64 2.8.5-4.el7 base 256 k audit-libs x86_64 2.8.5-4.el7 base 102 k policycoreutils x86_64 2.5-34.el7 base 917 kTransaction Summary===================================================================================================================================================================================================================Install 3 Packages (+8 Dependent packages)Upgrade ( 3 Dependent packages)Total download size: 91 MDownloading packages:No Presto metadata available for base(1/14): audit-libs-2.8.5-4.el7.x86_64.rpm | 102 kB 00:00:00(2/14): audit-libs-python-2.8.5-4.el7.x86_64.rpm | 76 kB 00:00:00(3/14): audit-2.8.5-4.el7.x86_64.rpm | 256 kB 00:00:00(4/14): checkpolicy-2.5-8.el7.x86_64.rpm | 295 kB 00:00:00(5/14): container-selinux-2.119.1-1.c57a6f9.el7.noarch.rpm | 40 kB 00:00:00warning: /var/cache/yum/x86_64/7/docker-ce-stable/packages/containerd.io-1.2.13-3.2.el7.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 621e9f35: NOKEY ] 2.0 MB/s | 45 MB 00:00:22 ETAPublic key for containerd.io-1.2.13-3.2.el7.x86_64.rpm is not installed(6/14): containerd.io-1.2.13-3.2.el7.x86_64.rpm | 25 MB 00:00:32(7/14): libcgroup-0.41-21.el7.x86_64.rpm | 66 kB 00:00:00(8/14): libsemanage-python-2.5-14.el7.x86_64.rpm | 113 kB 00:00:00(9/14): policycoreutils-python-2.5-34.el7.x86_64.rpm | 457 kB 00:00:01(10/14): python-IPy-0.75-6.el7.noarch.rpm | 32 kB 00:00:00(11/14): setools-libs-3.3.8-4.el7.x86_64.rpm | 620 kB 00:00:00(12/14): policycoreutils-2.5-34.el7.x86_64.rpm | 917 kB 00:00:02(13/14): docker-ce-19.03.11-3.el7.x86_64.rpm | 24 MB 00:00:36(14/14): docker-ce-cli-19.03.11-3.el7.x86_64.rpm | 38 MB 00:00:28-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Total 1.5 MB/s | 91 MB 00:01:01Retrieving key from https://download.docker.com/linux/centos/gpgImporting GPG key 0x621E9F35: Userid : \"Docker Release (CE rpm) &lt;docker@docker.com&gt;\" Fingerprint: 060a 61c5 1b55 8a7f 742b 77aa c52f eb6b 621e 9f35 From : https://download.docker.com/linux/centos/gpgRunning transaction checkRunning transaction testTransaction test succeededRunning transaction Updating : audit-libs-2.8.5-4.el7.x86_64 1/17 Updating : policycoreutils-2.5-34.el7.x86_64 2/17 Installing : libcgroup-0.41-21.el7.x86_64 3/17 Installing : audit-libs-python-2.8.5-4.el7.x86_64 4/17 Installing : python-IPy-0.75-6.el7.noarch 5/17 Installing : setools-libs-3.3.8-4.el7.x86_64 6/17 Installing : libsemanage-python-2.5-14.el7.x86_64 7/17 Installing : 1:docker-ce-cli-19.03.11-3.el7.x86_64 8/17 Installing : checkpolicy-2.5-8.el7.x86_64 9/17 Installing : policycoreutils-python-2.5-34.el7.x86_64 10/17 Installing : 2:container-selinux-2.119.1-1.c57a6f9.el7.noarch 11/17 Installing : containerd.io-1.2.13-3.2.el7.x86_64 12/17 Installing : 3:docker-ce-19.03.11-3.el7.x86_64 13/17 Updating : audit-2.8.5-4.el7.x86_64 14/17 Cleanup : policycoreutils-2.5-29.el7_6.1.x86_64 15/17 Cleanup : audit-2.8.4-4.el7.x86_64 16/17 Cleanup : audit-libs-2.8.4-4.el7.x86_64 17/17 Verifying : 2:container-selinux-2.119.1-1.c57a6f9.el7.noarch 1/17 Verifying : audit-libs-2.8.5-4.el7.x86_64 2/17 Verifying : checkpolicy-2.5-8.el7.x86_64 3/17 Verifying : audit-2.8.5-4.el7.x86_64 4/17 Verifying : 1:docker-ce-cli-19.03.11-3.el7.x86_64 5/17 Verifying : policycoreutils-2.5-34.el7.x86_64 6/17 Verifying : libsemanage-python-2.5-14.el7.x86_64 7/17 Verifying : containerd.io-1.2.13-3.2.el7.x86_64 8/17 Verifying : setools-libs-3.3.8-4.el7.x86_64 9/17 Verifying : python-IPy-0.75-6.el7.noarch 10/17 Verifying : policycoreutils-python-2.5-34.el7.x86_64 11/17 Verifying : audit-libs-python-2.8.5-4.el7.x86_64 12/17 Verifying : 3:docker-ce-19.03.11-3.el7.x86_64 13/17 Verifying : libcgroup-0.41-21.el7.x86_64 14/17 Verifying : audit-libs-2.8.4-4.el7.x86_64 15/17 Verifying : audit-2.8.4-4.el7.x86_64 16/17 Verifying : policycoreutils-2.5-29.el7_6.1.x86_64 17/17Installed: containerd.io.x86_64 0:1.2.13-3.2.el7 docker-ce.x86_64 3:19.03.11-3.el7 docker-ce-cli.x86_64 1:19.03.11-3.el7Dependency Installed: audit-libs-python.x86_64 0:2.8.5-4.el7 checkpolicy.x86_64 0:2.5-8.el7 container-selinux.noarch 2:2.119.1-1.c57a6f9.el7 libcgroup.x86_64 0:0.41-21.el7 libsemanage-python.x86_64 0:2.5-14.el7 policycoreutils-python.x86_64 0:2.5-34.el7 python-IPy.noarch 0:0.75-6.el7 setools-libs.x86_64 0:3.3.8-4.el7Dependency Updated: audit.x86_64 0:2.8.5-4.el7 audit-libs.x86_64 0:2.8.5-4.el7 policycoreutils.x86_64 0:2.5-34.el7Complete![root@localhost /]# 五、 将当前用户添加到docker用户组vagrant为登录用户名称，根据自己机器登录名称设定 script123[root@localhost /]# sudo usermod -aG docker vagrant[root@localhost /]# sudo usermod -aG docker root[root@localhost /]# 六、 设置docker镜像下载源，指定cgroup驱动查看/etc/docker下是否有daemon.json文件，如果有手动修改，如果没有则直接执行如下命令 查看daemon.json是否存在，我这里不存在，执行第2步增加docker配置 script1234[root@localhost /]# cd /etc/docker/[root@localhost docker]# lskey.json[root@localhost docker]# 增加docker配置，并查看配置https://1mbc3b4s.mirror.aliyuncs.com的获取 阿里云镜像地址如果之前启动过docker,修改完成之后需要重启sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker script123456789101112sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF' { \"registry-mirrors\": [\"https://1mbc3b4s.mirror.aliyuncs.com\"], \"exec-opts\": [\"native.cgroupdriver=systemd\"]} EOF[root@localhost docker]# cat /etc/docker/daemon.json{ \"registry-mirrors\": [\"https://1mbc3b4s.mirror.aliyuncs.com\"], \"exec-opts\": [\"native.cgroupdriver=systemd\"]}[root@localhost docker]# 七、 启动dockerscript12[root@localhost /]# sudo systemctl start docker[root@localhost /]# 八、 查看docker是否安装成功这里可以看到docker的版本为19.03.11，增加了docker0网络 script123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475[root@localhost /]# docker infoClient: Debug Mode: falseServer: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 19.03.11 Storage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: systemd Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: 7ad184331fa3e55e52b890ea95e65ba581ae3429 runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd init version: fec3683 Security Options: seccomp Profile: default Kernel Version: 3.10.0-957.12.2.el7.x86_64 Operating System: CentOS Linux 7 (Core) OSType: linux Architecture: x86_64 CPUs: 4 Total Memory: 3.7GiB Name: xincan.base.com ID: IQSQ:3C2Y:CYZ6:UQVE:VEOH:HCVJ:NNUS:W2E5:NLRE:GAME:NH2L:2XHC Docker Root Dir: /var/lib/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Registry Mirrors: https://1mbc3b4s.mirror.aliyuncs.com/ Live Restore Enabled: false[root@localhost /]#[root@localhost /]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:00:8a:fe:e6 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic eth0 valid_lft 70167sec preferred_lft 70167sec inet6 fe80::5054:ff:fe8a:fee6/64 scope link valid_lft forever preferred_lft forever3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 08:00:27:dd:2a:77 brd ff:ff:ff:ff:ff:ff inet 192.168.1.40/24 brd 192.168.1.255 scope global noprefixroute eth1 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fedd:2a77/64 scope link valid_lft forever preferred_lft forever4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:49:69:f4:4a brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever[root@localhost /]# 九、 设置docker开机启动script123[root@localhost /]# sudo systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.[root@localhost /]# 十、 安装docker-compose（墙裂建议） docker-compose安装文档地址 下载docker-compose sudo curl -L “https://github.com/docker/compose/releases/download/1.26.0/docker-compose-$(uname -s)-$(uname -m)” -o /usr/local/bin/docker-compose 赋予docker-compose权限 sudo chmod +x /usr/local/bin/docker-compose 添加docker-compose软连接 sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose script123456789101112[root@xincan /]# sudo curl -L \"https://github.com/docker/compose/releases/download/1.26.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 638 100 638 0 0 239 0 0:00:02 0:00:02 --:--:-- 239100 11.6M 100 11.6M 0 0 1208k 0 0:00:09 0:00:09 --:--:-- 2430k[root@xincan /]# docker-compose --version-bash: /usr/local/bin/docker-compose: Permission denied[root@xincan /]# sudo chmod +x /usr/local/bin/docker-compose[root@xincan /]# sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose[root@xincan /]# docker-compose --versiondocker-compose version 1.26.0, build d4451659[root@xincan /]# 十、 安装docker-compose（墙裂建议） 安装bash-completion yum install -y bash-completion 赋予docker-compose权限 sudo chmod +x /usr/local/bin/docker-compose 添加docker-compose软连接 sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose script1[root@xincan /]#yum install -y bash-completion","link":"/posts/cd183338/"},{"title":"Docker常用命令","text":"环境准备准备一台带docker的虚拟机，请参照centos构建docker基础环境 一、 docker swarm相关命令script1234567891011## 初始化集群主节点docker swarm init## 子节点加入集群中docker swarm join --token SWMTKN-1-4lzr2216s61ecbyayyqynjwybmxy5y5th5ru8aal2a0d1t2vn3-ekdgf4swlz8fiq4nnzgnbhr5u 192.168.1.10:2377## 退出集群docker swarm leave## 强制退出集群docker swarm leave --force 一、 docker node相关命令script123456## 查看集群节点信息docker node ls## 升级工作节点为管理节点docker node promote node1.draas.hatech.com.cndocker node promote node2.draas.hatech.com.cn 一、 docker network相关命令script12345## 查看网络列表docker network ls## 自定义overlay网络docker network create -d overlay my-overlay","link":"/posts/29dc6fe8/"},{"title":"github Issue 作为博客微型数据库的应用","text":"背景众所周知，对于hexo框架搭建的静态博客，难免会产生一些动态的数据，比如一些碎碎念、友链、音乐、时间轴等微型数据。目前一般的做法: a.是创建一个json数据，来存储这些微型数据，但是如果数据太多的话，一是比较慢，二是有个硬伤问题，就是json数据不能分页请求，只能一次拿完，太多的话网络带宽占用太多。 b.或者有的直接后台写一些接口服务啥的，还得在买个服务器部署上去，然后博客中访问接口。 c.或者有些可能就直接写到html中。 对于a、c方法都比较麻烦，每次更新了都要编译部署，不能很方便的动态更新。对于b的话，成本以及技术要求可能就更多一些了。 基于上面出现的问题，目前想到的一个解决方案就是，利用github 的issue作为一个微型数据库。能够很方便的动态更新，也能分页，也不需要啥json文件，想想都很方便。 issue数据库使用步骤issue的创建先创建一个Repository，对于此Repository可以专门作为微型的数据库，取名issue_database。创建好之后建立一些issue 如下所示 目前博客中，所有的动态数据都放到issue中了。 issue中存储数据对于创建好的issue，就可以往里面写数据了，比如我的友链数据为issue：blog_friends 对于issue中存储的数据最好存json格式的，因为可以方便后面取出来使用。存储好数据后，如果太多的话，可以点击hide,隐藏起来。同时这个issue最好给Lock conversation这样的好处是，防止别人往里面加些脏数据，只能自己往里写数据。哈哈，一般也没有闲的无聊的网友恶作剧。这样就存储好数据了。 博客中获取issue数据博客中通过js获取issue中的数据，以博客友链为例，以下是获取代码，以及处理 123456789101112131415161718// author by removef// https://removeif.github.io/$(function () { //获取处理友链数据，来自issue，一次取完 $.getJSON(\"https://api.github.com/repos/removeif/issue_database/issues/2/comments?per_page=100&amp;client_id=46a9f3481b46ea0129d8&amp;client_secret=79c7c9cb847e141757d7864453bcbf89f0655b24\", function (source) { var data = []; var source1; source1 = source; // 以后每次更新的都在后面，此处倒序，按时间降序排 source1.reverse(); // 把所有的数据放到data的列表中 $.each(source1, function (i, e) { data.push(...JSON.parse(e.body)); }); $.each(data, function (i, e) { // 博客中html文件的构建，渲染 });}); 上面代码中client_id、client_secret在另一篇文章中博客源码分享有详细的说明,可以查看一下。这样就能获取到相应的数据，进行操作。 issue数据的更新比如想更新任意一项数据都可以进github中对应的仓库的issue下进行更新，添加。然后实时去博客中查看。 扩展一下对于有些爱唠叨的人（比如我），弄个类似碎碎念的东西就比较实用了。之前想过各种方案，存json数据太不方便；后台写个服务部署服务器也太麻烦。最后思来想去还是利用了下现成的优秀项目gitalk,稍稍改改就能很好使用。 博客中的碎碎念对于博主而言，有发表框和修改的操作，能够方便发表和修改。 可能有时候还会发表一些图片，对图片的样式做了一些控制 对于网友的话只能查看以及点赞加❤️ 做法就是源码中改下返回html的文件内容，如果是管理员和非管理员返回一些不同的元素，能够很好的实现碎碎念的功能。查看碎碎念。 总结静态博客的动态数据是个痛点，GitHub Issue有很多可利用的地方。多去探索发掘其中的奥妙。 利用GitHub Issue来解决目前也是一种解决方法。希望后面会出现更好的解决方案。","link":"/posts/e7fe3cc/"},{"title":"博客中gitalk最新评论的获取","text":"博客中，对于网友的评论以及每篇文章的评论数还是很重要的。但是基于静态的页面想要存储动态的评论数据是比较难的，一般博客主题中都内置了评论插件，但是博客主题中对于最新评论的支持显示还是很少的，至少目前我是没怎么发现。博客 Powered by Hexo &amp; Icarus，采用Gitalk评论，再次感谢此三位作者的辛勤码代码，才有了以下的内容。基于此背景基础上，聊聊最新评论的实现。 博客的使用， Hexo &amp; Icarus，采用Gitalk评论 的使用自行百度了。 使用场景 最新评论列表 最热文章列表（基于评论数判断是否最热，也比较片面，但是侧面也能反映，问题不大） 使用方法主要参考自官方文档 目前主要用到两个方法，一个是获取仓库下所有的issue，每个issue节点下有相关的评论数，以及对应issue下的评论的url;还有一个是根据issue下评论的URL获取相应的所有的评论 方法1：List issues for a repository1GET /orgs/:org/issues 参数列表 Name Type Description milestone integer or string If an integer is passed, it should refer to a milestone by its number field. If the string * is passed, issues with any milestone are accepted. If the string none is passed, issues without milestones are returned. state string Indicates the state of the issues to return. Can be either open, closed, or all. Default: open assignee string Can be the name of a user. Pass in none for issues with no assigned user, and * for issues assigned to any user. creator string The user that created the issue. mentioned string A user that’s mentioned in the issue. labels string A list of comma separated label names. Example: bug,ui,@high sort string What to sort results by. Can be either created, updated, comments. Default: created direction string The direction of the sort. Can be either asc or desc. Default: desc since string Only issues updated at or after this time are returned. This is a timestamp in ISO 8601 format: YYYY-MM-DDTHH:MM:SSZ. 以上参数，主要用到 sort 排序，page页数，per_page每页数量，其余的参数看个人需要使用。注意文档中的说明，排序的字段和返回的稍许不太一样。 方法2：List comments on an issue1GET /repos/:owner/:repo/issues/:issue_number/comments Issue Comments are ordered by ascending ID. 排序根据 ascending (上升的，增长的；升（序）的)ID.也就是说，从老到新。这个比较坑，对于我们获取最新评论来说。 参数如下 Name Type Description since string Only comments updated at or after this time are returned. This is a timestamp in ISO 8601 format: YYYY-MM-DDTHH:MM:SSZ. 根据尝试以及以上参数，试出不支持排序，但是支持分页，page,per_page参数，对于我们获取最新的评论来说可以根据评论数，算出分页数，拿到最后一条，即最新一条 123456//如果只有一页int page = 1;int per_page = 1;// 如果超出一页的话int page = 2;int per_page = commentsNumber-1;//commentsNumber:评论数 js代码中使用实例核心代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758var timesSet = [];var timesBodyMap = {};var timesSetMap = {};var resultArr = [];// 方法1：sort=comments可以按评论数排序，此处更适合按更新时间排序,可以根据updated排序，但是0条评论的也会出来，所以此处还是根据评论数排序全部查出来，过滤掉0条评论的，拿到每个issue下最新的一条评论详情和时间，根据时间内存排序// per_page 每页数量，根据需求配置$.getJSON(\"https://api.github.com/repos/{用户名}/{仓库}/issues?per_page=100&amp;sort=comments\", function (result) { $.each(result, function (i, item) { var commentsCount = item.comments; if (commentsCount &gt; 0) { $.ajaxSettings.async = false; // 此处保证是最后一条，api没有排序参数，只能分页取最后一条，保证最少的数据量传输，快速处理 var page = 2; var pageSize = commentsCount - 1; if (commentsCount == 1) { page = 1; pageSize = 1; } // 方法2：的使用 $.getJSON(item.comments_url + \"?page=\" + page + \"&amp;per_page=\" + pageSize, function (commentResult) { var item1 = commentResult[0]; var contentStr = item1.body.trim(); if (contentStr.length &gt; 50) { contentStr = contentStr.substr(0, 60); contentStr += \"...\"; } timesSet.push(new Date(item1.created_at).getTime()); timesBodyMap[item1.created_at] = { \"title\": item.title.substr(0, item.title.indexOf(\"-\") - 1), \"url\": item.body.substr(0, item.body.indexOf(\"\\n\") - 1), \"content\": contentStr, \"date\": item1.created_at, \"userName\": item1[\"user\"].login, \"userUrl\": item1[\"user\"].html_url, \"commentCount\": commentsCount }; timesSetMap[new Date(item1.created_at).getTime()] = item1.created_at; }); } });});// 排序if (timesSet.length &gt; 0) { timesSet.sort();}// 根据需要取10条if (timesSet.length &gt; 10) { for (var i = timesSet.length - 1; i &gt;= 0 &amp;&amp; resultArr.length &lt; 10; i--) { resultArr.push(timesBodyMap[timesSetMap[timesSet[i]]]); }}else { for (var i = timesSet.length - 1; i &gt;= 0; i--) { resultArr.push(timesBodyMap[timesSetMap[timesSet[i]]]); }} 方法1：请求接口地址示例1https://api.github.com/repos/removeif/blog_comment/issues?per_page=100&amp;sort=comments 返回结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[ { \"url\": \"https://api.github.com/repos/removeif/blog_comment/issues/3\", \"repository_url\": \"https://api.github.com/repos/removeif/blog_comment\", \"labels_url\": \"https://api.github.com/repos/removeif/blog_comment/issues/3/labels{/name}\", \"comments_url\": \"https://api.github.com/repos/removeif/blog_comment/issues/3/comments\", \"events_url\": \"https://api.github.com/repos/removeif/blog_comment/issues/3/events\", \"html_url\": \"https://github.com/removeif/blog_comment/issues/3\", \"id\": 458985510, \"node_id\": \"MDU6SXNzdWU0NTg5ODU1MTA=\", \"number\": 3, \"title\": \"留言板 - 辣椒の酱\", \"user\": { \"login\": \"removeif\", \"id\": 10427139, \"node_id\": \"MDQ6VXNlcjEwNDI3MTM5\", \"avatar_url\": \"https://avatars1.githubusercontent.com/u/10427139?v=4\", \"gravatar_id\": \"\", \"url\": \"https://api.github.com/users/removeif\", \"html_url\": \"https://github.com/removeif\", \"followers_url\": \"https://api.github.com/users/removeif/followers\", \"following_url\": \"https://api.github.com/users/removeif/following{/other_user}\", \"gists_url\": \"https://api.github.com/users/removeif/gists{/gist_id}\", \"starred_url\": \"https://api.github.com/users/removeif/starred{/owner}{/repo}\", \"subscriptions_url\": \"https://api.github.com/users/removeif/subscriptions\", \"organizations_url\": \"https://api.github.com/users/removeif/orgs\", \"repos_url\": \"https://api.github.com/users/removeif/repos\", \"events_url\": \"https://api.github.com/users/removeif/events{/privacy}\", \"received_events_url\": \"https://api.github.com/users/removeif/received_events\", \"type\": \"User\", \"site_admin\": false }, \"labels\": [ { \"id\": 1416043904, \"node_id\": \"MDU6TGFiZWwxNDE2MDQzOTA0\", \"url\": \"https://api.github.com/repos/removeif/blog_comment/labels/3306ea6632b94cc388b40cef9dda4a8f\", \"name\": \"3306ea6632b94cc388b40cef9dda4a8f\", \"color\": \"0e8a16\", \"default\": false }, { \"id\": 1415994590, \"node_id\": \"MDU6TGFiZWwxNDE1OTk0NTkw\", \"url\": \"https://api.github.com/repos/removeif/blog_comment/labels/Gitalk\", \"name\": \"Gitalk\", \"color\": \"5319e7\", \"default\": false } ], \"state\": \"open\", \"locked\": false, \"assignee\": null, \"assignees\": [ ], \"milestone\": null, \"comments\": 33, \"created_at\": \"2019-06-21T03:06:53Z\", \"updated_at\": \"2019-09-12T10:37:34Z\", \"closed_at\": null, \"author_association\": \"OWNER\", \"body\": \"https://removeif.github.io/message/\\r\\n\\r\\n留言板信息。\" }, {...} ] 方法2：请求接口地址示例1https://api.github.com/repos/removeif/blog_comment/issues/3/comments?per_page=32&amp;page=2 返回结果 123456789101112131415161718192021222324252627282930313233[ { \"url\": \"https://api.github.com/repos/removeif/blog_comment/issues/comments/530767913\", \"html_url\": \"https://github.com/removeif/blog_comment/issues/3#issuecomment-530767913\", \"issue_url\": \"https://api.github.com/repos/removeif/blog_comment/issues/3\", \"id\": 530767913, \"node_id\": \"MDEyOklzc3VlQ29tbWVudDUzMDc2NzkxMw==\", \"user\": { \"login\": \"removeif\", \"id\": 10427139, \"node_id\": \"MDQ6VXNlcjEwNDI3MTM5\", \"avatar_url\": \"https://avatars1.githubusercontent.com/u/10427139?v=4\", \"gravatar_id\": \"\", \"url\": \"https://api.github.com/users/removeif\", \"html_url\": \"https://github.com/removeif\", \"followers_url\": \"https://api.github.com/users/removeif/followers\", \"following_url\": \"https://api.github.com/users/removeif/following{/other_user}\", \"gists_url\": \"https://api.github.com/users/removeif/gists{/gist_id}\", \"starred_url\": \"https://api.github.com/users/removeif/starred{/owner}{/repo}\", \"subscriptions_url\": \"https://api.github.com/users/removeif/subscriptions\", \"organizations_url\": \"https://api.github.com/users/removeif/orgs\", \"repos_url\": \"https://api.github.com/users/removeif/repos\", \"events_url\": \"https://api.github.com/users/removeif/events{/privacy}\", \"received_events_url\": \"https://api.github.com/users/removeif/received_events\", \"type\": \"User\", \"site_admin\": false }, \"created_at\": \"2019-09-12T10:37:34Z\", \"updated_at\": \"2019-09-12T10:37:34Z\", \"author_association\": \"OWNER\", \"body\": \"&gt; 哇 大佬你博客弄的好厉害啊 可以指点指点吗\\n&gt;&gt; @xuelangjing 还好吧😂，简简单单的，可以多看下网页上的源码，有什么问题可以讨论讨论哦\" }] 博客中目前有两个页面使用,根据个人的需要放到各自的位置吧。 首页热门推荐 还有个最新评论页： 扩展一个方法上面的实例程序，每个issue（因为我的每个issue关联一个文章链接）只取了一条最新的评论，假如每个issue下有两个都是最新的评论，而我也不管是不是同一个issue下的评论，获取所有的最新评论，还有一个方法比较好用。 List comments in a repository1GET /repos/:owner/:repo/issues/comments By default, Issue Comments are ordered by ascending ID. 和上面一样，但是以下参数就不一样了 Name Type Description sort string Either created or updated. Default: created direction string Either asc or desc. Ignored without the sort parameter. since string Only comments updated at or after this time are returned. This is a timestamp in ISO 8601 format: YYYY-MM-DDTHH:MM:SSZ. 多了排序字段和排序方式，也有per和per_page，这是相当的有用啊 扩展方法：请求接口地址示例1https://api.github.com/repos/removeif/blog_comment/issues/comments?sort=updated&amp;direction=desc&amp;per_page=10&amp;page=1 返回结果 123456789101112131415161718192021222324252627282930313233343536[ { \"url\": \"https://api.github.com/repos/removeif/blog_comment/issues/comments/530767913\", \"html_url\": \"https://github.com/removeif/blog_comment/issues/3#issuecomment-530767913\", \"issue_url\": \"https://api.github.com/repos/removeif/blog_comment/issues/3\", \"id\": 530767913, \"node_id\": \"MDEyOklzc3VlQ29tbWVudDUzMDc2NzkxMw==\", \"user\": { \"login\": \"removeif\", \"id\": 10427139, \"node_id\": \"MDQ6VXNlcjEwNDI3MTM5\", \"avatar_url\": \"https://avatars1.githubusercontent.com/u/10427139?v=4\", \"gravatar_id\": \"\", \"url\": \"https://api.github.com/users/removeif\", \"html_url\": \"https://github.com/removeif\", \"followers_url\": \"https://api.github.com/users/removeif/followers\", \"following_url\": \"https://api.github.com/users/removeif/following{/other_user}\", \"gists_url\": \"https://api.github.com/users/removeif/gists{/gist_id}\", \"starred_url\": \"https://api.github.com/users/removeif/starred{/owner}{/repo}\", \"subscriptions_url\": \"https://api.github.com/users/removeif/subscriptions\", \"organizations_url\": \"https://api.github.com/users/removeif/orgs\", \"repos_url\": \"https://api.github.com/users/removeif/repos\", \"events_url\": \"https://api.github.com/users/removeif/events{/privacy}\", \"received_events_url\": \"https://api.github.com/users/removeif/received_events\", \"type\": \"User\", \"site_admin\": false }, \"created_at\": \"2019-09-12T10:37:34Z\", \"updated_at\": \"2019-09-12T10:37:34Z\", \"author_association\": \"OWNER\", \"body\": \"&gt; 哇 大佬你博客弄的好厉害啊 可以指点指点吗\\n&gt;&gt; @xuelangjing 还好吧😂，简简单单的，可以多看下网页上的源码，有什么问题可以讨论讨论哦\" }, { ... } ] 总结此扩展方法优点：对于不在乎issue数量，只在乎最新评论的就比较适用，能够精准拿出前10条，很赞不足：一个issue下多个最新评论，如果想要显示的最新评论列表还包括文章标题，看起来可能不太好看，很多重复，但是看个人需要吧 注意事项，采坑环节 对应接口的请求限制，目前接口有请求的限制，所以使用中不能频繁请求，调试的时候一会儿又限制，一会儿又限制比较麻烦，限制十几分钟之后就解除了。 对于页面中，一般很多个地方可能都需要展示这个列表，所以不能每次都去请求，必须缓存起来，一般缓存到本地，我的是存的cookie中，十分钟去请求一次，所以调好后一般不会出现限制情况。但是马上评论了的就看不到，有10分钟的延迟，不过也还好。 对于如果issue以及评论太多的情况，尽量的少请求，比如上面的分页优化，取最后一条。以及页面中请求时做出异步请求的方式，不要阻止其他元素的渲染。 本人主要做后端，对前端的set/排序不太熟悉，上面实现排序代码比较繁琐😂，如果有什么更好的方法，麻烦也告知一下，互相学习共同进步。","link":"/posts/47c45f64/"},{"title":"不蒜子统计常见问题","text":"不蒜子统计官网：http://busuanzi.ibruce.info/ 详细使用教程：http://ibruce.info/2015/04/04/busuanzi/ 目前支持的功能：（两行代码，搞定计数；方便、简洁、实用） a、显示站点总访问量 b、显示单页面访问量 c、显示站点总访问量和单页面访问量 d、只计数不显示 关于怎么实现当天、昨天、本月、上月（即具体时间段）的访问量目前没有支持，请配合目前不蒜子支持的功能自行实现。 1.常见问题400错误，统计不生效 如图（1）的地方没有加载出统计数据 此时F12打开浏览器控制台，找到（2）network地方，刷新一下页面找到（3），请求统计的网址如图所示，查看（4）referrer-policy是否如图所示，如图的话是不能访问的，需要更改。 导致此问题原因，检查网页源码中，一般header有如下标签 解决方法，去掉此标签，之后访问如下（1）已有统计值，（2）已改变。可详细对比正常使用不蒜子统计网址https://removeif.github.io/ 中请求busuanzi?jsonpCallback=BusuanziCallback_236107382952地址的请求和返回参数的差异！ 这样设置带来的影响，可能有些图床的图片不能显示，会图裂，如新浪图床。解决方法，可以用其他不冲突的图床。 2.统计访问数巨大（清零问题）http://localhost:4000/ 或http://127.0.0.1:4000/ 访问时，统计数巨大，这是正常的，不用清零。部署到线上，用线上域名网址访问数据就正常了。 3.统计无法访问如图所示 查看红色url部分如下所示 此问题是Request URL填写错误，请确保Request URL前部分为http://busuanzi.ibruce.info/busuanzi?，，，，如下 同时查看Response，出现如下数据，就是成功访问了不蒜子统计，如果网站中还没出统计数据，就是自己的代码写错了，检查代码 4.其余问题(1).同一个页面，同名的id标签确保只能放一个如下 123&lt;span id=\"busuanzi_container_site_uv\" style=\"display: inline;\"&gt; &lt;span id=\"busuanzi_value_site_uv\"&gt;&lt;/span&gt;&lt;/span&gt; 同一个页面id名为busuanzi_value_site_uv只能放一个! (2).safari（包括移动端的safari）浏览器页面pv统计问题，如下，因为Safari浏览器referer在文章页面时也上送的域名（而单个页面的pv统计是根据页面路径，即上送的referer），所以此时页面的pv返回的站点的pv值。暂时没解决方法，可自行搜索解决方案构造正确的referer值。对于此问题可以换个浏览器看哇，比如Chrome，哈哈。 (3).部分live2d可能与不蒜子统计有冲突，出现此问题时，请查看网页源码引用统计id处是否被隐藏，一般网页上会自动加上display: none;自行选择性的使用。网友的解决方法，修改源码为以下，并引用修改后的js文件 不蒜子源码文件： 修改为如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889var bszCaller, bszTag;!function() { var c, d, e, a = !1, b = []; ready = function(c) { return a || \"interactive\" === document.readyState || \"complete\" === document.readyState ? c.call(document) : b.push(function() { return c.call(this) }), this } , d = function() { for (var a = 0, c = b.length; c &gt; a; a++) b[a].apply(document); b = [] } , e = function() { a || (a = !0, d.call(window), document.removeEventListener ? document.removeEventListener(\"DOMContentLoaded\", e, !1) : document.attachEvent &amp;&amp; (document.detachEvent(\"onreadystatechange\", e), window == window.top &amp;&amp; (clearInterval(c), c = null))) } , document.addEventListener ? document.addEventListener(\"DOMContentLoaded\", e, !1) : document.attachEvent &amp;&amp; (document.attachEvent(\"onreadystatechange\", function() { /loaded|complete/.test(document.readyState) &amp;&amp; e() }), window == window.top &amp;&amp; (c = setInterval(function() { try { a || document.documentElement.doScroll(\"left\") } catch (b) { return } e() }, 5)))}(),bszCaller = { fetch: function(a, b) { var c = \"BusuanziCallback_\" + Math.floor(1099511627776 * Math.random()); window[c] = this.evalCall(b), a = a.replace(\"=BusuanziCallback\", \"=\" + c), scriptTag = document.createElement(\"SCRIPT\"), scriptTag.type = \"text/javascript\", scriptTag.defer = !0, scriptTag.src = a, document.getElementsByTagName(\"HEAD\")[0].appendChild(scriptTag) }, evalCall: function(a) { return function(b) { ready(function() { try { a(b), - // 此处为修改的逻辑- scriptTag.parentElement.removeChild(scriptTag)+ if(scriptTag != null &amp;&amp; scriptTag.parentElement != null){+ scriptTag.parentElement.removeChild(scriptTag)+ } } catch (c) { bszTag.hides() } }) } }},bszCaller.fetch(\"//busuanzi.ibruce.info/busuanzi?jsonpCallback=BusuanziCallback\", function(a) { bszTag.texts(a), bszTag.shows()}),bszTag = { bszs: [\"site_pv\", \"page_pv\", \"site_uv\"], texts: function(a) { this.bszs.map(function(b) { var c = document.getElementById(\"busuanzi_value_\" + b); c &amp;&amp; (c.innerHTML = a[b]) }) }, hides: function() { this.bszs.map(function(a) { var b = document.getElementById(\"busuanzi_container_\" + a); b &amp;&amp; (b.style.display = \"none\") }) }, shows: function() { this.bszs.map(function(a) { var b = document.getElementById(\"busuanzi_container_\" + a); b &amp;&amp; (b.style.display = \"inline\") }) }}; 5.官方群！！！群号：419260983，有其余问题进群讨论，提问时遇到其他的问题请同时发出问题的在线网址！！！","link":"/posts/89986481/"},{"title":"博客图片上传picgo工具github图传使用","text":"摘要对于每一个写博客的人来说，图片是至关重要。这一路经历了多次图片的烦恼，之前选择了微博个人文章那里粘贴图片的方式上传，感觉也挺方便的。但是由于新浪的图片显示问题，如果header中不设置 标签就不能异步访问图片，导致图裂，那之恶心。然而设置之后又与网站访客统计的插件冲突，使之不能统计，真是神仙打架。无赖之下使用了PicGo工具，使用后感觉真XX方便！ PicGo工具下载安装配置下载 .PicGo下载 github网站提供三个版本的下载，MacOs、linux、windows覆盖市面上90%系统，还是很给力了。 我是mac用户，直接使用brew cask来安装PicGo: brew cask install picgo，简直方便到爆。 配置 PicGo配置(使用github图传，免费方便，同时配合github.io博客真是方便) 选上必填的就ok,一开始不知道token的设置，附赠token获取方法 图片上传相关的设置 链接格式：选择适合自己的，一般用户md文件中，选第一个，然后就可以疯狂使用了。 使用github图传，获取token在github-&gt;setting-&gt;developer settings 选择generate new token 勾选好之后生成就好了 使用 PicGo使用，简直方便 1).默认网页上直接右键复制图片 2).点击等待中的图片，开始上传 3).上传完之后有个提示，同时粘贴板也会自动粘贴上 4).直接粘贴到想要的地方 或者也可以直接截图，然后点击图片里的图片上传，很方便 PicGo上传动图gif 如果直接复制网页上的动图，去上传的话是截取的某帧，是静图。应该下载到本地，然后在拖进去上传就可以了。","link":"/posts/9e7c733f/"},{"title":"github page网站cdn优化加速","text":"CDN的全称是Content Delivery Network，即内容分发网络。CDN是构建在网络之上的内容分发网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN的关键技术主要有内容存储和分发技术。——百度百科 放在Github的资源在国内加载速度比较慢，因此需要使用CDN加速来优化网站打开速度，jsDelivr + Github便是免费且好用的CDN，非常适合博客网站使用。 图片加速关于图传以及GitHub作为图库的使用方法请参考文章：博客图片上传picgo工具github图传使用。 在上面参考文章的基础之上只需要修改以下配置：（指定相关cdn域名） 原来项目中使用了原来的方式，进行全局替换，Mac idea直接快捷键command+shift+R全局替换 【ps：题外话】原来是统一用的GitHub的仓库中的图片，通过这样替换，可以看到图片统一管理是多么的重要，多么的方便管理操作。 至此，博客中的相关图片都加上了cdn。 其余资源文件用法： 1https://cdn.jsdelivr.net/gh/你的用户名/你的仓库名@发布的版本号/文件路径 例如： 123https://cdn.jsdelivr.net/gh/TRHX/CDN-for-itrhx.com@1.0/images/trhx.pnghttps://cdn.jsdelivr.net/gh/TRHX/CDN-for-itrhx.com@2.0.1/css/style.csshttps://cdn.jsdelivr.net/gh/moezx/cdn@3.1.3//The%20Pet%20Girl%20of%20Sakurasou.mp4 注意：版本号不是必需的，是为了区分新旧资源，如果不使用版本号，将会直接引用最新资源，除此之外还可以使用某个范围内的版本，查看所有资源等，具体使用方法如下： 123456789101112131415161718// 加载任何Github发布、提交或分支https://cdn.jsdelivr.net/gh/user/repo@version/file// 加载 jQuery v3.2.1https://cdn.jsdelivr.net/gh/jquery/jquery@3.2.1/dist/jquery.min.js// 使用版本范围而不是特定版本https://cdn.jsdelivr.net/gh/jquery/jquery@3.2/dist/jquery.min.jshttps://cdn.jsdelivr.net/gh/jquery/jquery@3/dist/jquery.min.js// 完全省略该版本以获取最新版本https://cdn.jsdelivr.net/gh/jquery/jquery/dist/jquery.min.js// 将“.min”添加到任何JS/CSS文件中以获取缩小版本，如果不存在，将为会自动生成https://cdn.jsdelivr.net/gh/jquery/jquery@3.2.1/src/core.min.js// 在末尾添加 / 以获取资源目录列表https://cdn.jsdelivr.net/gh/jquery/jquery/ 至此，github page 博客基本需要加速的完成。 参考文章:参考链接1参考链接2","link":"/posts/346ce02e/"},{"title":"博客换肤的一种实现方式思路","text":"当博客内容很多的时候，比如需要加载很多资源文件，许多炫酷的东西的时候，可能相应的就是比较慢了（正可谓时间和空间不能兼得）。虽然目前也有很多方式手段可以提高访问速度，但是博客提供一个简洁模式还是很有必要的，萝卜青菜，各有所爱嘛。说不定很多网友就当纯的想看看文字，不需要那些花里胡哨的东西。这时候提供个清爽模式就相当有用了。 正常模式和精简模式hexo框架2仓2主题，采用正常模式一个仓库，一个主题；精简模式另一个仓库，另一个主题。 本博客采用的github Page部署网站。大家都知道，一个github的账户名，只能够指定一个username.github.io的网址，所以两个仓库，两个主题的话，就必须有一个挂在username.github.io之上，比如正常模式username.github.io，精简模式为username.github.io/name.io。 正常模式正常模式里面可以放各种炫酷的东西，提供丰富的页面。 精简模式只提供必要的文章，归档，分类，搜索基本的东西就够了。看个人需要，既然要简洁，就尽量的少弄一些。 配置方法关于_config.yml主配置文件的注意事项。 1234567891011+ root: /remove.io/ #精简模式- root: / #正常模式deploy: type: git+ repo: https://github.com/removeif/remove.io.git #精简模式- repo: https://github.com/removeif/removeif.github.io.git #正常模式+ theme: icarus #正常模式- theme: nextn #精简模式 对于root 根节点的说明，因为精简模式的所有资源文件都是挂在 username.github.io/remove.io/ 所以相当于根节点为/remove.io/ 总结注意事项 对于页面中对于对应模式下资源文件的引用，一定加上域名地址 ，比如原来图片访问/image/tuizi.jpg，在精简模式的时候如果继续这样用，就找不到，对应模式下的图片了，需要加上前面的username.github.io/remove.io/ 地址。 对于精简模式下，能去掉的东西就尽量去掉，尽量少加载一些，速度更快。 对于冲突页面的处理，一般对于文章或者关于页面都是通用。文章一般没啥影响，但是关于页面，可能有些也有很炫酷的模块。对于精简模式，可能不需要，此时就需要多new 一个page页面，分开配置，比如下面主题中的_config.yml配置。 12+ /remove.io/abouta/ #精简模式- /abouta/ #正常模式 本博客正常模式 精简模式 以上只是提供了一种解决方法思路，肯定还有更好的方式。","link":"/posts/1c5449cf/"},{"title":"安装、部分配置icarus主题中文版","text":"摘要发现icarus主题还不错，花了一两个小时研究了下安装、部分配置icarus主题中文版 安装icarus 直接下载主题模块放到blog项目 ,blog项目根目录执行 1git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus 此时已经下载到项目中。 顶级_config.yml中选择icarus主题 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: icarus 此时主题已经安装好，清除、编译、部署可以看到效果了 配置icarus 完全参照官网配置，进行翻译解说 配置文章部分顶部图片添加icarus 主题中的配置_config.yml中开启图片开关 12article: thumbnail: true 文章.md文件头中添加图片绝对/相对地址 12345title: Getting Started with Icarusthumbnail: /gallery/thumbnails/desert.jpg// thumbnail:https://cdn.jsdelivr.net/gh/removeif/blog_image/20190620152744.png---Post content... 配置完成后部署显示效果如下(最新文章列表显示缩略图、文章开头显示一张设置图片) 左边文章导航栏开启icarus 主题中的配置_config.yml中开关 1234widgets: - type: toc position: left 同事文章顶部加入标签 1234title: Table of Contents Exampletoc: true---Post content... 配置效果 评论系统开启icarus 主题中的配置_config.yml中开启（部分评论系统需要翻墙才能使用，valine不用翻墙个人推荐，valine安装参考） 1234567comment: type: valine app_id: xxxxxxxx # (required) LeanCloud application id app_key: xxxxxxxx # (required) LeanCloud application key notify: false # (optional) receive email notification verify: false # (optional) show verification code placeholder: xxxxxxxx # (optional) comment box placeholder text 开启效果 捐赠收款开启icarus 主题中的配置_config.yml中开启 注意如果默认不配置，编译时有报错，可以# 把它注释掉，不启用功能 1234567891011donate: - # Donation entry name type: alipay # Qrcode image URL qrcode: 'https://wx2.sinaimg.cn/large/b5d1b710gy1g0lvxdcwm0j20p011i4bg.jpg' - # Donation entry name type: wechat # Qrcode image URL qrcode: 'https://wx2.sinaimg.cn/large/b5d1b710gy1g0lvwdcpb5j20u014qgy2.jpg' 开启配置效果如下 全局搜索开启icarus 主题中的配置_config.yml中开启,不同的搜索类型需要安装插件参考官网,type: insight此类型不需要安装，已经内置 12search: type: insight 效果如下 更多配置请参考官网配置目前配置基本已经够使用，还需要更多配置请参考连接 参考自","link":"/posts/52c38f29/"},{"title":"博客源码分享","text":"写在前面 博客源码包括两个主题icarus和next，在主题基础之上参照各网友博客，以及自己的一些想法做出的一些修改以及增加部分新元素。 以下是修改后的需要的部分配置，其余的配置参照icarus主题配置和next主题配置。因为修改了原作者源码，有什么问题请先联系我，不要去麻烦原作者了，能自己解决的问题就不要麻烦别人了，每个人的时间都很宝贵。膜拜和感谢所有模块的原作者,orz👻,辛苦了。 欢迎围观：博客+主题源码、纯主题源码、博主博客 一、icarus主题之上主要改动 新增gitalk最新评论widget 首页增加热门推荐 增加弹性配置影音（可加音乐、视频）模块 丰富弹性配置about页面 新增弹性配置友链模块 整体布局左右拉伸了一点，紧凑一些 文章页双栏模式、固定导航栏 引入可配置看板娘 归档页加入了一个文章贡献概览 置顶文章的设置 文章列表评论数显示 文章中推荐文章模块配置 增加深色主题切换 加入加密文章 碎碎念功能 透明无界样式 简化部分widget数据，加入查看全部按钮 gitalk评论增加评论开关，评论列表中标记博主 还有什么新的，好的feature欢迎大家随时提出来，有能力有时间就做出来 二、部分配置说明：本机环境：12345678192:hexo-theme-icarus-removeif xx$ node -vv11.1.0192:hexo-theme-icarus-removeif xx$ npm -v6.4.1``` #### 克隆博客代码到本地```jshelllanguagegit clone https://github.com/removeif/hexo-theme-icarus-removeif.git 开始部分配置：敲黑板！！！！首先全局以及主题中的_config.yml配置成自己的对应参数。 1.热门推荐，最新评论：仅针对gitalk评论有效，如果配置完后显示本博客相关评论、推荐，请详细阅读这一条热门推荐，最新评论，文章评论数关联的js文件路径：themes/icarus/source/js/comment-issue-data.js以下引号里的地址改成自己对应的博客评论的issues的仓库相关的值。repoIssuesUrl改两个值（removeif和blog_comment改成自己对应的） themes/icarus/source/js/comment-issue-data.js12345// 评论issues仓库 by.removeif https://removeif.github.io/var repoIssuesUrl = \"https://api.github.com/repos/removeif/blog_comment/issues\"; // removeif：用户名，blog_comment：评论的issue仓库// 评论issues仓库 clientId、clientSecret怎么申请自行搜索，关于这暴露两个参数的安全问题，查看 https://removeif.github.io/2019/09/19/博客源码分享.html#1-热门推荐，最新评论：var clientId = \"46a9f3481b46ea0129d8\";var clientSecret = \"79c7c9cb847e141757d7864453bcbf89f0655b24\"; github api 详情可以参照官方api说明关于配置暴露client_id和client_secret安全性问题，gitalk作者有解释对应主题中的_config.yml要开启如下配置，xxx换成自己的，否则无效。 themes/icarus/_config.yml >folded1234567891011comment: type: gitalk owner: xxx # (required) GitHub user name repo: xxx # (required) GitHub repository name client_id: xxx # (required) OAuth application client id client_secret: xxx # (required) OAuth application client secret admin: xxx #此账户一般为用户名 GitHub user name 文章中能创建issue需要此用户登录才可以，点了创建issue后刷新一遍才能看到！！！！ create_issue_manually: true distraction_free_mode: true has_hot_recommend: true # 是否有热门推荐 has_latest_comment: true #是否有最新评论 说明： has_hot_recommend: true 是否开启首页热评，false-不开启，true-开启 has_latest_comment: true 是否开启最新评论，false-不开启，true-开启 热门推荐数据为评论数最多的文章，🔥后面的数字：根据文章的评论数*101 。 最新评论：为该仓库下，所有issue中的最新评论。 目前的最新评论有1分钟的本地缓存，评论后可能1分钟后才能看见最新评论，出于性能优化，每次请求接口处理还是挺耗时，comment-issue-data.js中可以自己去掉。 2.友链数据文件：文件路径：themes/icarus/source/js/friend.js相应格式增加自己需要的数据。 3.影音数据文件：文件路径：音乐：themes/icarus/source/json_data/music.json视频：themes/icarus/source/json_data/video.json相应格式增加自己需要的数据。 4.关于页面时间轴记录数据文件：文件路径：themes/icarus/source/json_data/record.json相应格式增加自己需要的数据。 5.看板娘配置主题中的_config.yml配置如下设置 1live2Dswitch: off #live2D开关 on为打开,off为关闭 6.置顶设置：.md文章头部数据中加入top值，top值越大越靠前，大于0显示置顶图标。修改依赖包中文件removeif/node_modules/hexo-generator-index/lib/generator.js如下： node_modules/hexo-generator-index/lib/generator.js >folded12345678910111213141516171819202122232425262728'use strict';const pagination = require('hexo-pagination');module.exports = function(locals){ var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) { if(a.top == undefined){ a.top = 0; } if(b.top == undefined){ b.top = 0; } if(a.top == b.top){ return b.date - a.date; }else{ return b.top - a.top; } }); var paginationDir = config.pagination_dir || 'page'; return pagination('', posts, { perPage: config.index_generator.per_page, layout: ['index', 'archive'], format: paginationDir + '/%d/', data: { __index: true } });}; 7.配置文章中推荐文章模块根据配置的recommend值（必须大于0），值越大越靠前，相等取最新的，最多取5条。recommend（6.中top值也在下面示例）配置在.md文章头中，如下 123456789title: 博客源码分享top: 1toc: truerecommend: 1 keywords: categories-githubdate: 2019-09-19 22:10:43thumbnail: https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20190919221611.pngtags: 工具教程categories: [工具教程,主题工具] 8.文章中某个代码块折叠的方法代码块头部加入标记 &gt;folded，如下代码块中使用。 main.java >folded123456789// 使用示例，.md 文件中头行标记\"&gt;folded\"// ```java main.java &gt;folded// import main.java// private static void main(){// // test// int i = 0;// return i;// }// \\\\``` 9.加入加密文章如下需要加密的文章头部加入以下代码 1234567891011121314---title: 2019成长记01top: -1toc: truekeywords: categories-java#以下为文章加密信息encrypt: truepassword: 123456 #此处为文章密码abstract: 咦，这是一篇加密文章，好像需要输入密码才能查看呢！message: 嗨，请准确无误地输入密码查看哟！wrong_pass_message: 不好意思，密码没对哦，在检查检查呢！wrong_hash_message: 不好意思，信息无法验证！--- 注：加密文章不会出现在最新文章列表widget中，也不会出现在文章中推荐列表中，首页列表中需要设置top: -1 让它排在最后比较合理一些。 10.碎碎念的使用在github中，创建碎碎念issue，并且打上对应的label（eg:666666）对应配置中为id，填写到source/self-talking/index.md文件中如下对应位置，其余配置也要改成自己的，如clientID等。 12345678910111213&lt;script&gt; var gitalk = new Gitalk({ clientID: '46a9f3481b46ea0129d8', clientSecret: '79c7c9cb847e141757d7864453bcbf89f0655b24', id: '666666', repo: 'issue_database', owner: 'removeif', admin: \"removeif\", createIssueManually: true, distractionFreeMode: false }) gitalk.render('comment-container1')&lt;/script&gt; 如下： 11.本博客样式（透明无界）只需要放开themes/icarus/source/css/base.styl文件中以下样式代码注释即可，默认是注释的没启用 base.styl >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//=================本博客使用样式 start// 首页去图.body_hot_comment .comment-content .card-comment-item .ava, .media-left, .is-6-widescreen .card-image { display: none;}hover-color = #deeafb;// 去card.card { background-color: unset; //box-shadow: unset;}.navbar, footer.footer { background-color: unset;}body:not(.night) .navbar:hover,body:not(.night) .footer:hover,body:not(.night) .card:hover,body:not(.night) .pagination:hover,body:not(.night) .post-navigation:hover{ background-color: hover-color; box-shadow: 0 4px 10px rgba(0,0,0,0.05),0 0 1px rgba(0,0,0,0.1);}.pagination, .post-navigation{ padding: 10px;}.pagination .pagination-link:not(.is-current), .pagination .pagination-previous, .pagination .pagination-next { background-color:rgba(255,255,255,0);}.timeline .media:last-child:after { background: unset;}.footer { box-shadow: 0px 4px 10px 10px rgba(0,0,0,0.05); padding: 3rem 1.5rem 2rem;}@media screen and (max-width: 1087px) .navbar-menu { background-color: unset; }//=================本博客使用样式 end 如下： 精简部分widget数据widget中的归档和分类和标签精简了，数据多时很丑，改为了分别展示5条和10条和20条，增加了查看全部。 gitalk评论增加评论开关，评论列表中标记博主需要关闭评论的在文章头部加入 comments: false,原来已经评论的依然会显示，如下 原来已有博客文章的迁移，只需要把原来对应的文章放到source/_posts里即可。然后去对应文章下面创建评论issue。 以上配置好后12345$ npm install #安装依赖包（只需要执行一次）ps:如果是纯主题仓库，可直接把本文最后的json文件复制到博客下面的依赖文件package.json后在执行此命令$ hexo clean #清除缓存$ hexo g #编译 $ hexo s #启动服务 $ hexo d #推到远程 安装依赖包（只需要执行一次），以后修改了代码 只需要执行后面几条就好。 ok,enjoy it!！👏👏 有什么问题，欢迎issue里讨论。 写在后面如果你有问题请反馈: issues （请务必先于issues中寻找答案）如果你喜欢该主题: star如果你想定制主题: fork 文章中横竖图demo；对于横竖图推荐分开使用，且长宽一致的，如统一手机拍照、电脑截图使用方法：md文章中放入以下代码 index.html>folded123456789101112131415161718+ 横竖图&lt;div class=\"justified-gallery\"&gt;![张芷溪](http://wx1.sinaimg.cn/large/b5d1b710ly1g6bz7n92s7j212w0nr1kx.jpg) ![李一桐](http://wx2.sinaimg.cn/mw1024/005RAHfgly1fvfc4f19qfj33402c0qv9.jpg) ![gakki](http://wx1.sinaimg.cn/mw1024/70396e5agy1g5qe457i6yj21660ogtap.jpg) ![李一桐](http://wx1.sinaimg.cn/mw1024/005RAHfgly1fuzz17s2q3j32e43cku0x.jpg) ![彭小苒](http://wx1.sinaimg.cn/mw1024/d79c9b94ly1g1pb1uthr5j21f02iox6t.jpg)&lt;/div&gt;+ 横图4&lt;div class=\"img-x\"&gt;![v4](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191022182226.png) ![v3](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191018114126.png) ![v4](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191022182226.png) ![v3](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191018114126.png)&lt;/div&gt;+ 竖图5&lt;div class=\"img-y\"&gt;![电池](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191024145940.jpg) ![打王者荣耀](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191024141906.jpg) ![支付宝付款](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191024141926.jpg) ![锤子便签](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191024145956.jpg) ![电池](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191024145940.jpg)&lt;/div&gt; 效果如下（多图左右拉查看） 横竖图 横图4 竖图5 博客快照： 主页 深色主题 置顶 文章评论数 推荐文章模块 归档 留言 友链 美图 影音 关于 提供hexo博客目录下依赖包 package.json1234567891011121314151617181920212223242526272829303132333435363738394041{ \"name\": \"hexo-site\", \"version\": \"3.0.0\", \"private\": true, \"scripts\": { \"build\": \"hexo generate\", \"clean\": \"hexo clean\", \"deploy\": \"hexo deploy\", \"server\": \"hexo server\" }, \"hexo\": { \"version\": \"4.2.0\" }, \"dependencies\": { \"ajv\": \"^6.10.2\", \"bulma-stylus\": \"0.8.0\", \"deepmerge\": \"^4.2.2\", \"hexo\": \"^4.2.0\", \"hexo-blog-encrypt\": \"^3.0.3\", \"hexo-deployer-git\": \"^2.1.0\", \"hexo-generator-archive\": \"^1.0.0\", \"hexo-generator-category\": \"^1.0.0\", \"hexo-generator-feed\": \"^2.2.0\", \"hexo-generator-index\": \"^1.0.0\", \"hexo-generator-tag\": \"^1.0.0\", \"hexo-log\": \"^1.0.0\", \"hexo-pagination\": \"^1.0.0\", \"hexo-renderer-ejs\": \"^1.0.0\", \"hexo-renderer-inferno\": \"^0.1.1\", \"hexo-renderer-marked\": \"^2.0.0\", \"hexo-renderer-stylus\": \"^1.1.0\", \"hexo-server\": \"^1.0.0\", \"hexo-util\": \"^1.8.0\", \"inferno\": \"^7.3.3\", \"inferno-create-element\": \"^7.3.3\", \"js-yaml\": \"^3.13.1\", \"moment\": \"^2.22.2\", \"save\": \"^2.4.0\", \"semver\": \"&gt;=5.0.0\" }}","link":"/posts/1a54ada2/"}],"tags":[{"name":"加密文章","slug":"加密文章","link":"/tags/%E5%8A%A0%E5%AF%86%E6%96%87%E7%AB%A0/"},{"name":"系统教程","slug":"系统教程","link":"/tags/%E7%B3%BB%E7%BB%9F%E6%95%99%E7%A8%8B/"},{"name":"开发教程","slug":"开发教程","link":"/tags/%E5%BC%80%E5%8F%91%E6%95%99%E7%A8%8B/"},{"name":"工具教程","slug":"工具教程","link":"/tags/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/"},{"name":"icarus主题配置","slug":"icarus主题配置","link":"/tags/icarus%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/"},{"name":"hexo主题","slug":"hexo主题","link":"/tags/hexo%E4%B8%BB%E9%A2%98/"},{"name":"博客统计插件","slug":"博客统计插件","link":"/tags/%E5%8D%9A%E5%AE%A2%E7%BB%9F%E8%AE%A1%E6%8F%92%E4%BB%B6/"}],"categories":[{"name":"private","slug":"private","link":"/categories/private/"},{"name":"系统教程","slug":"系统教程","link":"/categories/%E7%B3%BB%E7%BB%9F%E6%95%99%E7%A8%8B/"},{"name":"环境构建","slug":"系统教程/环境构建","link":"/categories/%E7%B3%BB%E7%BB%9F%E6%95%99%E7%A8%8B/%E7%8E%AF%E5%A2%83%E6%9E%84%E5%BB%BA/"},{"name":"开发教程","slug":"系统教程/环境构建/开发教程","link":"/categories/%E7%B3%BB%E7%BB%9F%E6%95%99%E7%A8%8B/%E7%8E%AF%E5%A2%83%E6%9E%84%E5%BB%BA/%E5%BC%80%E5%8F%91%E6%95%99%E7%A8%8B/"},{"name":"工具教程","slug":"工具教程","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/"},{"name":"主题工具","slug":"工具教程/主题工具","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E4%B8%BB%E9%A2%98%E5%B7%A5%E5%85%B7/"},{"name":"博客统计插件","slug":"工具教程/博客统计插件","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E5%8D%9A%E5%AE%A2%E7%BB%9F%E8%AE%A1%E6%8F%92%E4%BB%B6/"}]}